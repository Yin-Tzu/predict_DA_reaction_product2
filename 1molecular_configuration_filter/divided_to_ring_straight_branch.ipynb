{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "divided_to_ring_straight_branch.ipynb",
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Yin-Tzu/predict_DA_reaction_product2/blob/main/1molecular_configuration_filter/divided_to_ring_straight_branch.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1o3bqKT6yaWM"
      },
      "source": [
        "import zipfile\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras import layers\n",
        "import matplotlib.pyplot as plt\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import time\n",
        "from sklearn import metrics\n",
        "from sklearn.preprocessing import MinMaxScaler\n",
        "import cv2\n",
        "import copy\n",
        "from sklearn.model_selection import train_test_split\n",
        "xy=np.load('NN.zip')\n",
        "X_train, y_train_label, X_test, y_test_label,X_valid,y_valid_label=xy['X_train'],xy['y_train_label'],xy['X_test'],xy['y_test_label'],xy['X_valid'],xy['y_valid_label']\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uJI9B8RZwCmn",
        "outputId": "8a481cde-28b3-461a-9389-c3d79495d9b8"
      },
      "source": [
        "y_TrainOneHot = tf.keras.utils.to_categorical(y_train_label)  # One-Hot编码\n",
        "y_TestOneHot = tf.keras.utils.to_categorical(y_test_label)\n",
        "y_ValidOneHot = tf.keras.utils.to_categorical(y_valid_label)  # One-Hot编码\n",
        "print(X_train.shape, y_train_label.shape, X_test.shape, y_test_label.shape,X_valid.shape,y_valid_label.shape)\n",
        "tStart = time.time()#計時開始\n",
        "\n",
        "model = tf.keras.models.Sequential()  # 调用Sequential模型\n",
        "model.add(layers.Conv2D(input_shape=(300,300, 3), filters=16, kernel_size=(3,3), kernel_initializer='TruncatedNormal', strides=1, padding='same', activation='relu', name='conv1'))  # 10*10\n",
        "model.add(layers.AveragePooling2D(pool_size=(3,3), strides=2, padding='same', name='pool1'))  # 5*5\n",
        "model.add(layers.Conv2D(filters=16, kernel_size=(3,3), kernel_initializer='TruncatedNormal', strides=1, padding='same', activation='relu', name='conv3'))  # 10*10\n",
        "model.add(layers.MaxPooling2D(pool_size=(3,3), strides=2, padding='same', name='pool3'))\n",
        "model.add(layers.Conv2D(filters=32, kernel_size=(3,3), kernel_initializer='TruncatedNormal', strides=1, padding='same', activation='relu', name='conv2'))  # 10*10\n",
        "model.add(layers.MaxPooling2D(pool_size=(3,3), strides=2, padding='same', name='pool2'))  # 5*5\n",
        "model.add(layers.Conv2D(filters=64, kernel_size=(3,3), kernel_initializer='TruncatedNormal', strides=1, padding='same', activation='relu', name='conv4'))  # 10*10\n",
        "model.add(layers.MaxPooling2D(pool_size=(3,3), strides=2, padding='same', name='pool4'))  # 5*5\n",
        "model.add(layers.Conv2D(filters=64, kernel_size=(3,3), kernel_initializer='TruncatedNormal', strides=1, padding='same', activation='relu', name='conv5'))  # 10*10\n",
        "model.add(layers.MaxPooling2D(pool_size=(3,3), strides=2, padding='same', name='pool5'))  # 5*5\n",
        "#model.add(layers.Flatten(name='flatten'))\n",
        "#model.add(layers.Dense(units=64, kernel_initializer='TruncatedNormal', activation='relu'))\n",
        "model.add(layers.GlobalAveragePooling2D())\n",
        "model.add(layers.Dropout(0.2))  #在 0 和 1 之间浮动。需要丢弃的输入比例。\n",
        "model.add(layers.Dense(units=3,kernel_initializer='TruncatedNormal', activation='softmax'))#,input_dim=100\n",
        "\n",
        "print(model.summary())\n",
        "#batch_size = 2\n",
        "# 模型的训练 编译模型\n",
        "opt = tf.keras.optimizers.Adam(learning_rate=0.00005)# 3*3\n",
        "model.compile(loss='categorical_crossentropy', optimizer=opt, metrics=['accuracy'])  # metrics是训练和测试期间的模型评估标准。\n",
        "\n",
        "# 监控val_loss，当连续40轮变化小于0.0001时启动early stopping\n",
        "#es = tf.keras.callbacks.EarlyStopping(monitor='val_loss', mode='min', verbose=1, patience=40, min_delta=0.0001)\n",
        "\n",
        "# 训练模型\n",
        "train_history = model.fit(x=X_train, y=y_TrainOneHot, validation_data=(X_valid, y_ValidOneHot), epochs=400, batch_size=100, verbose=2)\n",
        "#train_history = model.fit_generator(generator(X_train, y_train_label, batch_size), epochs=100, steps_per_epoch=len(X_train)// batch_size,validation_data=generator(X_valid, y_valid_label, batch_size),validation_steps=len(X_valid) // batch_size, verbose=2,workers=5, use_multiprocessing=True)#validation_data=generator(X_test, y_test_label, batch_size),validation_steps=len(X_test) // batch_size\n",
        "# 查看训练过程，之前的训练步骤的值都保存在这里面。这里共有loss,accuracy,val_loss,val_accuracy四个参数\n",
        "print(train_history.history)\n",
        "\n",
        "# 將模型儲存至 HDF5 檔案中\n",
        "model.save('my_model2.h5')  # creates a HDF5 file 'my_model.h5'\n",
        "\n",
        "scores = model.evaluate(X_test,y_TestOneHot)\n",
        "#scores = model.evaluate_generator(generator(X_test, y_test_label, batch_size),steps=len(X_test)// batch_size)\n",
        "print('loss, accuracy=',scores) #显示测试准确率[1]\n",
        "\n",
        "prediction = model.predict_classes(X_test)\n",
        "# 返回预测属于某标签的概率\n",
        "y_score = model.predict_proba(X_test)\n",
        "\n",
        "t2 = time.time()#計時結束\n",
        "#列印結果\n",
        "print(\"It cost %f sec\" % (t2 - tStart))  #會自動做近位\n",
        "print(t2 - tStart)  #原型長這樣"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(5400, 300, 300, 3) (5400,) (1800, 300, 300, 3) (1800,) (1800, 300, 300, 3) (1800,)\n",
            "Model: \"sequential\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "conv1 (Conv2D)               (None, 300, 300, 16)      448       \n",
            "_________________________________________________________________\n",
            "pool1 (AveragePooling2D)     (None, 150, 150, 16)      0         \n",
            "_________________________________________________________________\n",
            "conv3 (Conv2D)               (None, 150, 150, 16)      2320      \n",
            "_________________________________________________________________\n",
            "pool3 (MaxPooling2D)         (None, 75, 75, 16)        0         \n",
            "_________________________________________________________________\n",
            "conv2 (Conv2D)               (None, 75, 75, 32)        4640      \n",
            "_________________________________________________________________\n",
            "pool2 (MaxPooling2D)         (None, 38, 38, 32)        0         \n",
            "_________________________________________________________________\n",
            "conv4 (Conv2D)               (None, 38, 38, 64)        18496     \n",
            "_________________________________________________________________\n",
            "pool4 (MaxPooling2D)         (None, 19, 19, 64)        0         \n",
            "_________________________________________________________________\n",
            "conv5 (Conv2D)               (None, 19, 19, 64)        36928     \n",
            "_________________________________________________________________\n",
            "pool5 (MaxPooling2D)         (None, 10, 10, 64)        0         \n",
            "_________________________________________________________________\n",
            "global_average_pooling2d (Gl (None, 64)                0         \n",
            "_________________________________________________________________\n",
            "dropout (Dropout)            (None, 64)                0         \n",
            "_________________________________________________________________\n",
            "dense (Dense)                (None, 3)                 195       \n",
            "=================================================================\n",
            "Total params: 63,027\n",
            "Trainable params: 63,027\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n",
            "None\n",
            "Epoch 1/400\n",
            "54/54 - 39s - loss: 1.1043 - accuracy: 0.3678 - val_loss: 1.0510 - val_accuracy: 0.4917\n",
            "Epoch 2/400\n",
            "54/54 - 6s - loss: 1.0202 - accuracy: 0.4872 - val_loss: 0.9390 - val_accuracy: 0.5322\n",
            "Epoch 3/400\n",
            "54/54 - 6s - loss: 0.8847 - accuracy: 0.5652 - val_loss: 0.7751 - val_accuracy: 0.6761\n",
            "Epoch 4/400\n",
            "54/54 - 6s - loss: 0.7827 - accuracy: 0.6131 - val_loss: 0.6994 - val_accuracy: 0.7033\n",
            "Epoch 5/400\n",
            "54/54 - 6s - loss: 0.7375 - accuracy: 0.6391 - val_loss: 0.6688 - val_accuracy: 0.6761\n",
            "Epoch 6/400\n",
            "54/54 - 6s - loss: 0.7113 - accuracy: 0.6528 - val_loss: 0.6541 - val_accuracy: 0.7122\n",
            "Epoch 7/400\n",
            "54/54 - 6s - loss: 0.7012 - accuracy: 0.6639 - val_loss: 0.6341 - val_accuracy: 0.7150\n",
            "Epoch 8/400\n",
            "54/54 - 6s - loss: 0.6794 - accuracy: 0.6846 - val_loss: 0.6206 - val_accuracy: 0.7306\n",
            "Epoch 9/400\n",
            "54/54 - 6s - loss: 0.6674 - accuracy: 0.6902 - val_loss: 0.6117 - val_accuracy: 0.7039\n",
            "Epoch 10/400\n",
            "54/54 - 6s - loss: 0.6614 - accuracy: 0.6931 - val_loss: 0.5952 - val_accuracy: 0.7417\n",
            "Epoch 11/400\n",
            "54/54 - 6s - loss: 0.6482 - accuracy: 0.7076 - val_loss: 0.5847 - val_accuracy: 0.7606\n",
            "Epoch 12/400\n",
            "54/54 - 6s - loss: 0.6414 - accuracy: 0.7135 - val_loss: 0.5790 - val_accuracy: 0.7733\n",
            "Epoch 13/400\n",
            "54/54 - 6s - loss: 0.6303 - accuracy: 0.7146 - val_loss: 0.5694 - val_accuracy: 0.7711\n",
            "Epoch 14/400\n",
            "54/54 - 6s - loss: 0.6239 - accuracy: 0.7226 - val_loss: 0.5736 - val_accuracy: 0.7572\n",
            "Epoch 15/400\n",
            "54/54 - 6s - loss: 0.6128 - accuracy: 0.7302 - val_loss: 0.5564 - val_accuracy: 0.7839\n",
            "Epoch 16/400\n",
            "54/54 - 6s - loss: 0.6047 - accuracy: 0.7374 - val_loss: 0.5517 - val_accuracy: 0.7817\n",
            "Epoch 17/400\n",
            "54/54 - 6s - loss: 0.6101 - accuracy: 0.7283 - val_loss: 0.5415 - val_accuracy: 0.7956\n",
            "Epoch 18/400\n",
            "54/54 - 6s - loss: 0.5980 - accuracy: 0.7369 - val_loss: 0.5435 - val_accuracy: 0.7867\n",
            "Epoch 19/400\n",
            "54/54 - 6s - loss: 0.5895 - accuracy: 0.7446 - val_loss: 0.5275 - val_accuracy: 0.7978\n",
            "Epoch 20/400\n",
            "54/54 - 6s - loss: 0.5801 - accuracy: 0.7476 - val_loss: 0.5334 - val_accuracy: 0.7856\n",
            "Epoch 21/400\n",
            "54/54 - 6s - loss: 0.5794 - accuracy: 0.7500 - val_loss: 0.5186 - val_accuracy: 0.7989\n",
            "Epoch 22/400\n",
            "54/54 - 6s - loss: 0.5693 - accuracy: 0.7565 - val_loss: 0.5091 - val_accuracy: 0.8100\n",
            "Epoch 23/400\n",
            "54/54 - 6s - loss: 0.5612 - accuracy: 0.7570 - val_loss: 0.5072 - val_accuracy: 0.7978\n",
            "Epoch 24/400\n",
            "54/54 - 6s - loss: 0.5528 - accuracy: 0.7633 - val_loss: 0.5003 - val_accuracy: 0.8050\n",
            "Epoch 25/400\n",
            "54/54 - 6s - loss: 0.5442 - accuracy: 0.7644 - val_loss: 0.4889 - val_accuracy: 0.8144\n",
            "Epoch 26/400\n",
            "54/54 - 6s - loss: 0.5371 - accuracy: 0.7663 - val_loss: 0.4833 - val_accuracy: 0.8106\n",
            "Epoch 27/400\n",
            "54/54 - 6s - loss: 0.5314 - accuracy: 0.7717 - val_loss: 0.4917 - val_accuracy: 0.8006\n",
            "Epoch 28/400\n",
            "54/54 - 6s - loss: 0.5233 - accuracy: 0.7765 - val_loss: 0.4751 - val_accuracy: 0.8111\n",
            "Epoch 29/400\n",
            "54/54 - 6s - loss: 0.5112 - accuracy: 0.7848 - val_loss: 0.4637 - val_accuracy: 0.8172\n",
            "Epoch 30/400\n",
            "54/54 - 6s - loss: 0.5015 - accuracy: 0.7828 - val_loss: 0.4566 - val_accuracy: 0.8256\n",
            "Epoch 31/400\n",
            "54/54 - 6s - loss: 0.4985 - accuracy: 0.7902 - val_loss: 0.4574 - val_accuracy: 0.8244\n",
            "Epoch 32/400\n",
            "54/54 - 6s - loss: 0.4877 - accuracy: 0.7946 - val_loss: 0.4481 - val_accuracy: 0.8200\n",
            "Epoch 33/400\n",
            "54/54 - 6s - loss: 0.4867 - accuracy: 0.7900 - val_loss: 0.4308 - val_accuracy: 0.8294\n",
            "Epoch 34/400\n",
            "54/54 - 6s - loss: 0.4779 - accuracy: 0.8050 - val_loss: 0.4233 - val_accuracy: 0.8289\n",
            "Epoch 35/400\n",
            "54/54 - 6s - loss: 0.4712 - accuracy: 0.7994 - val_loss: 0.4179 - val_accuracy: 0.8383\n",
            "Epoch 36/400\n",
            "54/54 - 6s - loss: 0.4582 - accuracy: 0.8122 - val_loss: 0.4131 - val_accuracy: 0.8417\n",
            "Epoch 37/400\n",
            "54/54 - 6s - loss: 0.4498 - accuracy: 0.8156 - val_loss: 0.3973 - val_accuracy: 0.8461\n",
            "Epoch 38/400\n",
            "54/54 - 6s - loss: 0.4554 - accuracy: 0.8120 - val_loss: 0.4318 - val_accuracy: 0.8183\n",
            "Epoch 39/400\n",
            "54/54 - 6s - loss: 0.4309 - accuracy: 0.8141 - val_loss: 0.3820 - val_accuracy: 0.8611\n",
            "Epoch 40/400\n",
            "54/54 - 6s - loss: 0.4226 - accuracy: 0.8274 - val_loss: 0.3803 - val_accuracy: 0.8589\n",
            "Epoch 41/400\n",
            "54/54 - 6s - loss: 0.4101 - accuracy: 0.8348 - val_loss: 0.3779 - val_accuracy: 0.8561\n",
            "Epoch 42/400\n",
            "54/54 - 6s - loss: 0.4046 - accuracy: 0.8330 - val_loss: 0.3688 - val_accuracy: 0.8561\n",
            "Epoch 43/400\n",
            "54/54 - 6s - loss: 0.3908 - accuracy: 0.8376 - val_loss: 0.3462 - val_accuracy: 0.8722\n",
            "Epoch 44/400\n",
            "54/54 - 6s - loss: 0.3922 - accuracy: 0.8381 - val_loss: 0.3364 - val_accuracy: 0.8800\n",
            "Epoch 45/400\n",
            "54/54 - 6s - loss: 0.3757 - accuracy: 0.8457 - val_loss: 0.3375 - val_accuracy: 0.8656\n",
            "Epoch 46/400\n",
            "54/54 - 6s - loss: 0.3616 - accuracy: 0.8596 - val_loss: 0.3240 - val_accuracy: 0.8767\n",
            "Epoch 47/400\n",
            "54/54 - 6s - loss: 0.3512 - accuracy: 0.8580 - val_loss: 0.3058 - val_accuracy: 0.8844\n",
            "Epoch 48/400\n",
            "54/54 - 6s - loss: 0.3572 - accuracy: 0.8511 - val_loss: 0.3155 - val_accuracy: 0.8772\n",
            "Epoch 49/400\n",
            "54/54 - 6s - loss: 0.3366 - accuracy: 0.8624 - val_loss: 0.2986 - val_accuracy: 0.8944\n",
            "Epoch 50/400\n",
            "54/54 - 6s - loss: 0.3266 - accuracy: 0.8733 - val_loss: 0.2867 - val_accuracy: 0.9017\n",
            "Epoch 51/400\n",
            "54/54 - 6s - loss: 0.3222 - accuracy: 0.8767 - val_loss: 0.2804 - val_accuracy: 0.9017\n",
            "Epoch 52/400\n",
            "54/54 - 6s - loss: 0.3087 - accuracy: 0.8820 - val_loss: 0.2701 - val_accuracy: 0.9039\n",
            "Epoch 53/400\n",
            "54/54 - 6s - loss: 0.3003 - accuracy: 0.8806 - val_loss: 0.2754 - val_accuracy: 0.9033\n",
            "Epoch 54/400\n",
            "54/54 - 6s - loss: 0.2956 - accuracy: 0.8848 - val_loss: 0.2685 - val_accuracy: 0.9083\n",
            "Epoch 55/400\n",
            "54/54 - 6s - loss: 0.2890 - accuracy: 0.8930 - val_loss: 0.2442 - val_accuracy: 0.9122\n",
            "Epoch 56/400\n",
            "54/54 - 6s - loss: 0.2771 - accuracy: 0.8924 - val_loss: 0.2422 - val_accuracy: 0.9183\n",
            "Epoch 57/400\n",
            "54/54 - 6s - loss: 0.2659 - accuracy: 0.9050 - val_loss: 0.2521 - val_accuracy: 0.9156\n",
            "Epoch 58/400\n",
            "54/54 - 6s - loss: 0.2664 - accuracy: 0.8972 - val_loss: 0.2297 - val_accuracy: 0.9233\n",
            "Epoch 59/400\n",
            "54/54 - 6s - loss: 0.2600 - accuracy: 0.9041 - val_loss: 0.2202 - val_accuracy: 0.9211\n",
            "Epoch 60/400\n",
            "54/54 - 6s - loss: 0.2593 - accuracy: 0.9019 - val_loss: 0.2367 - val_accuracy: 0.9189\n",
            "Epoch 61/400\n",
            "54/54 - 6s - loss: 0.2464 - accuracy: 0.9063 - val_loss: 0.2670 - val_accuracy: 0.9072\n",
            "Epoch 62/400\n",
            "54/54 - 6s - loss: 0.2485 - accuracy: 0.9087 - val_loss: 0.2084 - val_accuracy: 0.9306\n",
            "Epoch 63/400\n",
            "54/54 - 6s - loss: 0.2371 - accuracy: 0.9083 - val_loss: 0.2077 - val_accuracy: 0.9294\n",
            "Epoch 64/400\n",
            "54/54 - 6s - loss: 0.2231 - accuracy: 0.9154 - val_loss: 0.1892 - val_accuracy: 0.9317\n",
            "Epoch 65/400\n",
            "54/54 - 6s - loss: 0.2198 - accuracy: 0.9157 - val_loss: 0.1936 - val_accuracy: 0.9411\n",
            "Epoch 66/400\n",
            "54/54 - 6s - loss: 0.2204 - accuracy: 0.9165 - val_loss: 0.1837 - val_accuracy: 0.9422\n",
            "Epoch 67/400\n",
            "54/54 - 6s - loss: 0.2126 - accuracy: 0.9233 - val_loss: 0.1902 - val_accuracy: 0.9322\n",
            "Epoch 68/400\n",
            "54/54 - 6s - loss: 0.2088 - accuracy: 0.9254 - val_loss: 0.1804 - val_accuracy: 0.9467\n",
            "Epoch 69/400\n",
            "54/54 - 6s - loss: 0.2090 - accuracy: 0.9217 - val_loss: 0.1785 - val_accuracy: 0.9450\n",
            "Epoch 70/400\n",
            "54/54 - 6s - loss: 0.2056 - accuracy: 0.9274 - val_loss: 0.1717 - val_accuracy: 0.9450\n",
            "Epoch 71/400\n",
            "54/54 - 6s - loss: 0.1923 - accuracy: 0.9328 - val_loss: 0.1654 - val_accuracy: 0.9378\n",
            "Epoch 72/400\n",
            "54/54 - 6s - loss: 0.1832 - accuracy: 0.9372 - val_loss: 0.2203 - val_accuracy: 0.9011\n",
            "Epoch 73/400\n",
            "54/54 - 6s - loss: 0.1867 - accuracy: 0.9293 - val_loss: 0.1606 - val_accuracy: 0.9478\n",
            "Epoch 74/400\n",
            "54/54 - 6s - loss: 0.1774 - accuracy: 0.9339 - val_loss: 0.1579 - val_accuracy: 0.9494\n",
            "Epoch 75/400\n",
            "54/54 - 6s - loss: 0.1761 - accuracy: 0.9383 - val_loss: 0.1499 - val_accuracy: 0.9483\n",
            "Epoch 76/400\n",
            "54/54 - 6s - loss: 0.1712 - accuracy: 0.9419 - val_loss: 0.1677 - val_accuracy: 0.9372\n",
            "Epoch 77/400\n",
            "54/54 - 6s - loss: 0.1728 - accuracy: 0.9363 - val_loss: 0.1499 - val_accuracy: 0.9456\n",
            "Epoch 78/400\n",
            "54/54 - 6s - loss: 0.1707 - accuracy: 0.9398 - val_loss: 0.1483 - val_accuracy: 0.9506\n",
            "Epoch 79/400\n",
            "54/54 - 6s - loss: 0.1664 - accuracy: 0.9415 - val_loss: 0.1511 - val_accuracy: 0.9483\n",
            "Epoch 80/400\n",
            "54/54 - 6s - loss: 0.1738 - accuracy: 0.9372 - val_loss: 0.1636 - val_accuracy: 0.9356\n",
            "Epoch 81/400\n",
            "54/54 - 6s - loss: 0.1547 - accuracy: 0.9450 - val_loss: 0.1392 - val_accuracy: 0.9556\n",
            "Epoch 82/400\n",
            "54/54 - 6s - loss: 0.1664 - accuracy: 0.9439 - val_loss: 0.1454 - val_accuracy: 0.9544\n",
            "Epoch 83/400\n",
            "54/54 - 6s - loss: 0.1495 - accuracy: 0.9483 - val_loss: 0.1370 - val_accuracy: 0.9550\n",
            "Epoch 84/400\n",
            "54/54 - 6s - loss: 0.1480 - accuracy: 0.9465 - val_loss: 0.1517 - val_accuracy: 0.9461\n",
            "Epoch 85/400\n",
            "54/54 - 6s - loss: 0.1475 - accuracy: 0.9485 - val_loss: 0.1276 - val_accuracy: 0.9556\n",
            "Epoch 86/400\n",
            "54/54 - 6s - loss: 0.1543 - accuracy: 0.9483 - val_loss: 0.1571 - val_accuracy: 0.9344\n",
            "Epoch 87/400\n",
            "54/54 - 6s - loss: 0.1520 - accuracy: 0.9487 - val_loss: 0.1353 - val_accuracy: 0.9522\n",
            "Epoch 88/400\n",
            "54/54 - 6s - loss: 0.1375 - accuracy: 0.9530 - val_loss: 0.1217 - val_accuracy: 0.9611\n",
            "Epoch 89/400\n",
            "54/54 - 6s - loss: 0.1309 - accuracy: 0.9596 - val_loss: 0.1194 - val_accuracy: 0.9611\n",
            "Epoch 90/400\n",
            "54/54 - 6s - loss: 0.1381 - accuracy: 0.9513 - val_loss: 0.1246 - val_accuracy: 0.9600\n",
            "Epoch 91/400\n",
            "54/54 - 6s - loss: 0.1409 - accuracy: 0.9509 - val_loss: 0.1244 - val_accuracy: 0.9600\n",
            "Epoch 92/400\n",
            "54/54 - 6s - loss: 0.1275 - accuracy: 0.9572 - val_loss: 0.1139 - val_accuracy: 0.9611\n",
            "Epoch 93/400\n",
            "54/54 - 6s - loss: 0.1284 - accuracy: 0.9593 - val_loss: 0.1145 - val_accuracy: 0.9594\n",
            "Epoch 94/400\n",
            "54/54 - 6s - loss: 0.1243 - accuracy: 0.9598 - val_loss: 0.1150 - val_accuracy: 0.9650\n",
            "Epoch 95/400\n",
            "54/54 - 6s - loss: 0.1370 - accuracy: 0.9528 - val_loss: 0.1364 - val_accuracy: 0.9478\n",
            "Epoch 96/400\n",
            "54/54 - 6s - loss: 0.1280 - accuracy: 0.9574 - val_loss: 0.1133 - val_accuracy: 0.9678\n",
            "Epoch 97/400\n",
            "54/54 - 6s - loss: 0.1206 - accuracy: 0.9596 - val_loss: 0.1066 - val_accuracy: 0.9661\n",
            "Epoch 98/400\n",
            "54/54 - 6s - loss: 0.1210 - accuracy: 0.9596 - val_loss: 0.1774 - val_accuracy: 0.9406\n",
            "Epoch 99/400\n",
            "54/54 - 6s - loss: 0.1152 - accuracy: 0.9591 - val_loss: 0.1068 - val_accuracy: 0.9617\n",
            "Epoch 100/400\n",
            "54/54 - 6s - loss: 0.1149 - accuracy: 0.9606 - val_loss: 0.1031 - val_accuracy: 0.9683\n",
            "Epoch 101/400\n",
            "54/54 - 6s - loss: 0.1125 - accuracy: 0.9624 - val_loss: 0.1039 - val_accuracy: 0.9661\n",
            "Epoch 102/400\n",
            "54/54 - 6s - loss: 0.1113 - accuracy: 0.9652 - val_loss: 0.1073 - val_accuracy: 0.9706\n",
            "Epoch 103/400\n",
            "54/54 - 6s - loss: 0.1063 - accuracy: 0.9626 - val_loss: 0.1062 - val_accuracy: 0.9667\n",
            "Epoch 104/400\n",
            "54/54 - 6s - loss: 0.1060 - accuracy: 0.9672 - val_loss: 0.0984 - val_accuracy: 0.9728\n",
            "Epoch 105/400\n",
            "54/54 - 6s - loss: 0.1208 - accuracy: 0.9585 - val_loss: 0.1040 - val_accuracy: 0.9611\n",
            "Epoch 106/400\n",
            "54/54 - 6s - loss: 0.1041 - accuracy: 0.9646 - val_loss: 0.0992 - val_accuracy: 0.9717\n",
            "Epoch 107/400\n",
            "54/54 - 6s - loss: 0.1064 - accuracy: 0.9663 - val_loss: 0.0990 - val_accuracy: 0.9706\n",
            "Epoch 108/400\n",
            "54/54 - 6s - loss: 0.1018 - accuracy: 0.9657 - val_loss: 0.0966 - val_accuracy: 0.9694\n",
            "Epoch 109/400\n",
            "54/54 - 6s - loss: 0.1092 - accuracy: 0.9648 - val_loss: 0.0996 - val_accuracy: 0.9733\n",
            "Epoch 110/400\n",
            "54/54 - 6s - loss: 0.0988 - accuracy: 0.9683 - val_loss: 0.0979 - val_accuracy: 0.9694\n",
            "Epoch 111/400\n",
            "54/54 - 6s - loss: 0.1034 - accuracy: 0.9665 - val_loss: 0.0994 - val_accuracy: 0.9733\n",
            "Epoch 112/400\n",
            "54/54 - 6s - loss: 0.0909 - accuracy: 0.9724 - val_loss: 0.0986 - val_accuracy: 0.9717\n",
            "Epoch 113/400\n",
            "54/54 - 6s - loss: 0.1009 - accuracy: 0.9672 - val_loss: 0.0898 - val_accuracy: 0.9700\n",
            "Epoch 114/400\n",
            "54/54 - 6s - loss: 0.0971 - accuracy: 0.9704 - val_loss: 0.0957 - val_accuracy: 0.9711\n",
            "Epoch 115/400\n",
            "54/54 - 6s - loss: 0.0954 - accuracy: 0.9704 - val_loss: 0.0982 - val_accuracy: 0.9644\n",
            "Epoch 116/400\n",
            "54/54 - 6s - loss: 0.0981 - accuracy: 0.9667 - val_loss: 0.0893 - val_accuracy: 0.9656\n",
            "Epoch 117/400\n",
            "54/54 - 6s - loss: 0.0937 - accuracy: 0.9698 - val_loss: 0.1063 - val_accuracy: 0.9633\n",
            "Epoch 118/400\n",
            "54/54 - 6s - loss: 0.0967 - accuracy: 0.9670 - val_loss: 0.0836 - val_accuracy: 0.9772\n",
            "Epoch 119/400\n",
            "54/54 - 6s - loss: 0.0895 - accuracy: 0.9706 - val_loss: 0.0898 - val_accuracy: 0.9728\n",
            "Epoch 120/400\n",
            "54/54 - 6s - loss: 0.0914 - accuracy: 0.9696 - val_loss: 0.0902 - val_accuracy: 0.9717\n",
            "Epoch 121/400\n",
            "54/54 - 6s - loss: 0.0938 - accuracy: 0.9715 - val_loss: 0.0850 - val_accuracy: 0.9783\n",
            "Epoch 122/400\n",
            "54/54 - 6s - loss: 0.0852 - accuracy: 0.9700 - val_loss: 0.0806 - val_accuracy: 0.9783\n",
            "Epoch 123/400\n",
            "54/54 - 6s - loss: 0.0911 - accuracy: 0.9691 - val_loss: 0.0950 - val_accuracy: 0.9761\n",
            "Epoch 124/400\n",
            "54/54 - 6s - loss: 0.0899 - accuracy: 0.9733 - val_loss: 0.0857 - val_accuracy: 0.9756\n",
            "Epoch 125/400\n",
            "54/54 - 6s - loss: 0.0812 - accuracy: 0.9728 - val_loss: 0.0892 - val_accuracy: 0.9711\n",
            "Epoch 126/400\n",
            "54/54 - 6s - loss: 0.0831 - accuracy: 0.9735 - val_loss: 0.0880 - val_accuracy: 0.9744\n",
            "Epoch 127/400\n",
            "54/54 - 6s - loss: 0.0827 - accuracy: 0.9706 - val_loss: 0.0807 - val_accuracy: 0.9794\n",
            "Epoch 128/400\n",
            "54/54 - 6s - loss: 0.0807 - accuracy: 0.9743 - val_loss: 0.0993 - val_accuracy: 0.9678\n",
            "Epoch 129/400\n",
            "54/54 - 6s - loss: 0.0781 - accuracy: 0.9772 - val_loss: 0.0765 - val_accuracy: 0.9783\n",
            "Epoch 130/400\n",
            "54/54 - 6s - loss: 0.0838 - accuracy: 0.9741 - val_loss: 0.0942 - val_accuracy: 0.9628\n",
            "Epoch 131/400\n",
            "54/54 - 6s - loss: 0.0740 - accuracy: 0.9767 - val_loss: 0.0825 - val_accuracy: 0.9750\n",
            "Epoch 132/400\n",
            "54/54 - 6s - loss: 0.0739 - accuracy: 0.9781 - val_loss: 0.0711 - val_accuracy: 0.9811\n",
            "Epoch 133/400\n",
            "54/54 - 6s - loss: 0.0687 - accuracy: 0.9785 - val_loss: 0.0774 - val_accuracy: 0.9783\n",
            "Epoch 134/400\n",
            "54/54 - 6s - loss: 0.0846 - accuracy: 0.9739 - val_loss: 0.0709 - val_accuracy: 0.9794\n",
            "Epoch 135/400\n",
            "54/54 - 6s - loss: 0.0748 - accuracy: 0.9759 - val_loss: 0.0778 - val_accuracy: 0.9794\n",
            "Epoch 136/400\n",
            "54/54 - 6s - loss: 0.0656 - accuracy: 0.9789 - val_loss: 0.0735 - val_accuracy: 0.9806\n",
            "Epoch 137/400\n",
            "54/54 - 6s - loss: 0.0708 - accuracy: 0.9785 - val_loss: 0.0738 - val_accuracy: 0.9806\n",
            "Epoch 138/400\n",
            "54/54 - 6s - loss: 0.0710 - accuracy: 0.9776 - val_loss: 0.0904 - val_accuracy: 0.9706\n",
            "Epoch 139/400\n",
            "54/54 - 6s - loss: 0.0713 - accuracy: 0.9783 - val_loss: 0.0861 - val_accuracy: 0.9700\n",
            "Epoch 140/400\n",
            "54/54 - 6s - loss: 0.0671 - accuracy: 0.9796 - val_loss: 0.0702 - val_accuracy: 0.9811\n",
            "Epoch 141/400\n",
            "54/54 - 6s - loss: 0.0621 - accuracy: 0.9819 - val_loss: 0.0666 - val_accuracy: 0.9806\n",
            "Epoch 142/400\n",
            "54/54 - 6s - loss: 0.0641 - accuracy: 0.9778 - val_loss: 0.0804 - val_accuracy: 0.9783\n",
            "Epoch 143/400\n",
            "54/54 - 6s - loss: 0.0706 - accuracy: 0.9785 - val_loss: 0.0667 - val_accuracy: 0.9828\n",
            "Epoch 144/400\n",
            "54/54 - 6s - loss: 0.0678 - accuracy: 0.9774 - val_loss: 0.0723 - val_accuracy: 0.9811\n",
            "Epoch 145/400\n",
            "54/54 - 6s - loss: 0.0666 - accuracy: 0.9806 - val_loss: 0.0638 - val_accuracy: 0.9800\n",
            "Epoch 146/400\n",
            "54/54 - 6s - loss: 0.0614 - accuracy: 0.9819 - val_loss: 0.0649 - val_accuracy: 0.9800\n",
            "Epoch 147/400\n",
            "54/54 - 6s - loss: 0.0542 - accuracy: 0.9843 - val_loss: 0.0630 - val_accuracy: 0.9817\n",
            "Epoch 148/400\n",
            "54/54 - 6s - loss: 0.0669 - accuracy: 0.9765 - val_loss: 0.0779 - val_accuracy: 0.9794\n",
            "Epoch 149/400\n",
            "54/54 - 6s - loss: 0.0622 - accuracy: 0.9815 - val_loss: 0.0648 - val_accuracy: 0.9833\n",
            "Epoch 150/400\n",
            "54/54 - 6s - loss: 0.0629 - accuracy: 0.9820 - val_loss: 0.0643 - val_accuracy: 0.9822\n",
            "Epoch 151/400\n",
            "54/54 - 6s - loss: 0.0593 - accuracy: 0.9813 - val_loss: 0.0680 - val_accuracy: 0.9783\n",
            "Epoch 152/400\n",
            "54/54 - 6s - loss: 0.0536 - accuracy: 0.9828 - val_loss: 0.0566 - val_accuracy: 0.9856\n",
            "Epoch 153/400\n",
            "54/54 - 6s - loss: 0.0550 - accuracy: 0.9835 - val_loss: 0.0627 - val_accuracy: 0.9850\n",
            "Epoch 154/400\n",
            "54/54 - 6s - loss: 0.0497 - accuracy: 0.9852 - val_loss: 0.0618 - val_accuracy: 0.9844\n",
            "Epoch 155/400\n",
            "54/54 - 6s - loss: 0.0593 - accuracy: 0.9802 - val_loss: 0.0599 - val_accuracy: 0.9850\n",
            "Epoch 156/400\n",
            "54/54 - 6s - loss: 0.0508 - accuracy: 0.9848 - val_loss: 0.0552 - val_accuracy: 0.9833\n",
            "Epoch 157/400\n",
            "54/54 - 6s - loss: 0.0517 - accuracy: 0.9828 - val_loss: 0.0617 - val_accuracy: 0.9811\n",
            "Epoch 158/400\n",
            "54/54 - 6s - loss: 0.0581 - accuracy: 0.9830 - val_loss: 0.0604 - val_accuracy: 0.9806\n",
            "Epoch 159/400\n",
            "54/54 - 6s - loss: 0.0522 - accuracy: 0.9833 - val_loss: 0.0562 - val_accuracy: 0.9833\n",
            "Epoch 160/400\n",
            "54/54 - 6s - loss: 0.0555 - accuracy: 0.9844 - val_loss: 0.0602 - val_accuracy: 0.9833\n",
            "Epoch 161/400\n",
            "54/54 - 6s - loss: 0.0572 - accuracy: 0.9835 - val_loss: 0.0748 - val_accuracy: 0.9750\n",
            "Epoch 162/400\n",
            "54/54 - 6s - loss: 0.0565 - accuracy: 0.9824 - val_loss: 0.0733 - val_accuracy: 0.9800\n",
            "Epoch 163/400\n",
            "54/54 - 6s - loss: 0.0452 - accuracy: 0.9869 - val_loss: 0.0739 - val_accuracy: 0.9817\n",
            "Epoch 164/400\n",
            "54/54 - 6s - loss: 0.0559 - accuracy: 0.9830 - val_loss: 0.0639 - val_accuracy: 0.9822\n",
            "Epoch 165/400\n",
            "54/54 - 6s - loss: 0.0448 - accuracy: 0.9869 - val_loss: 0.0552 - val_accuracy: 0.9850\n",
            "Epoch 166/400\n",
            "54/54 - 6s - loss: 0.0499 - accuracy: 0.9848 - val_loss: 0.0556 - val_accuracy: 0.9844\n",
            "Epoch 167/400\n",
            "54/54 - 6s - loss: 0.0478 - accuracy: 0.9869 - val_loss: 0.0537 - val_accuracy: 0.9861\n",
            "Epoch 168/400\n",
            "54/54 - 6s - loss: 0.0455 - accuracy: 0.9865 - val_loss: 0.0663 - val_accuracy: 0.9833\n",
            "Epoch 169/400\n",
            "54/54 - 6s - loss: 0.0483 - accuracy: 0.9854 - val_loss: 0.0480 - val_accuracy: 0.9867\n",
            "Epoch 170/400\n",
            "54/54 - 6s - loss: 0.0498 - accuracy: 0.9843 - val_loss: 0.0584 - val_accuracy: 0.9839\n",
            "Epoch 171/400\n",
            "54/54 - 6s - loss: 0.0434 - accuracy: 0.9872 - val_loss: 0.0574 - val_accuracy: 0.9822\n",
            "Epoch 172/400\n",
            "54/54 - 6s - loss: 0.0473 - accuracy: 0.9872 - val_loss: 0.0517 - val_accuracy: 0.9861\n",
            "Epoch 173/400\n",
            "54/54 - 6s - loss: 0.0456 - accuracy: 0.9870 - val_loss: 0.0822 - val_accuracy: 0.9756\n",
            "Epoch 174/400\n",
            "54/54 - 6s - loss: 0.0453 - accuracy: 0.9848 - val_loss: 0.0481 - val_accuracy: 0.9861\n",
            "Epoch 175/400\n",
            "54/54 - 6s - loss: 0.0386 - accuracy: 0.9891 - val_loss: 0.0477 - val_accuracy: 0.9856\n",
            "Epoch 176/400\n",
            "54/54 - 6s - loss: 0.0565 - accuracy: 0.9830 - val_loss: 0.0739 - val_accuracy: 0.9767\n",
            "Epoch 177/400\n",
            "54/54 - 6s - loss: 0.0432 - accuracy: 0.9869 - val_loss: 0.0507 - val_accuracy: 0.9833\n",
            "Epoch 178/400\n",
            "54/54 - 6s - loss: 0.0413 - accuracy: 0.9874 - val_loss: 0.0534 - val_accuracy: 0.9839\n",
            "Epoch 179/400\n",
            "54/54 - 6s - loss: 0.0390 - accuracy: 0.9893 - val_loss: 0.0520 - val_accuracy: 0.9861\n",
            "Epoch 180/400\n",
            "54/54 - 6s - loss: 0.0457 - accuracy: 0.9870 - val_loss: 0.0565 - val_accuracy: 0.9839\n",
            "Epoch 181/400\n",
            "54/54 - 6s - loss: 0.0386 - accuracy: 0.9896 - val_loss: 0.0552 - val_accuracy: 0.9844\n",
            "Epoch 182/400\n",
            "54/54 - 6s - loss: 0.0384 - accuracy: 0.9878 - val_loss: 0.0486 - val_accuracy: 0.9872\n",
            "Epoch 183/400\n",
            "54/54 - 6s - loss: 0.0358 - accuracy: 0.9891 - val_loss: 0.0519 - val_accuracy: 0.9883\n",
            "Epoch 184/400\n",
            "54/54 - 6s - loss: 0.0433 - accuracy: 0.9872 - val_loss: 0.0543 - val_accuracy: 0.9822\n",
            "Epoch 185/400\n",
            "54/54 - 6s - loss: 0.0409 - accuracy: 0.9865 - val_loss: 0.0604 - val_accuracy: 0.9828\n",
            "Epoch 186/400\n",
            "54/54 - 6s - loss: 0.0377 - accuracy: 0.9880 - val_loss: 0.0421 - val_accuracy: 0.9872\n",
            "Epoch 187/400\n",
            "54/54 - 6s - loss: 0.0327 - accuracy: 0.9893 - val_loss: 0.0500 - val_accuracy: 0.9867\n",
            "Epoch 188/400\n",
            "54/54 - 6s - loss: 0.0379 - accuracy: 0.9898 - val_loss: 0.0476 - val_accuracy: 0.9844\n",
            "Epoch 189/400\n",
            "54/54 - 6s - loss: 0.0348 - accuracy: 0.9906 - val_loss: 0.0470 - val_accuracy: 0.9878\n",
            "Epoch 190/400\n",
            "54/54 - 6s - loss: 0.0329 - accuracy: 0.9911 - val_loss: 0.0815 - val_accuracy: 0.9750\n",
            "Epoch 191/400\n",
            "54/54 - 6s - loss: 0.0346 - accuracy: 0.9907 - val_loss: 0.0491 - val_accuracy: 0.9872\n",
            "Epoch 192/400\n",
            "54/54 - 6s - loss: 0.0469 - accuracy: 0.9861 - val_loss: 0.0417 - val_accuracy: 0.9883\n",
            "Epoch 193/400\n",
            "54/54 - 6s - loss: 0.0370 - accuracy: 0.9889 - val_loss: 0.0409 - val_accuracy: 0.9872\n",
            "Epoch 194/400\n",
            "54/54 - 6s - loss: 0.0332 - accuracy: 0.9904 - val_loss: 0.0461 - val_accuracy: 0.9883\n",
            "Epoch 195/400\n",
            "54/54 - 6s - loss: 0.0365 - accuracy: 0.9900 - val_loss: 0.0451 - val_accuracy: 0.9883\n",
            "Epoch 196/400\n",
            "54/54 - 6s - loss: 0.0320 - accuracy: 0.9906 - val_loss: 0.0670 - val_accuracy: 0.9783\n",
            "Epoch 197/400\n",
            "54/54 - 6s - loss: 0.0327 - accuracy: 0.9898 - val_loss: 0.0482 - val_accuracy: 0.9839\n",
            "Epoch 198/400\n",
            "54/54 - 6s - loss: 0.0351 - accuracy: 0.9891 - val_loss: 0.0480 - val_accuracy: 0.9839\n",
            "Epoch 199/400\n",
            "54/54 - 6s - loss: 0.0322 - accuracy: 0.9906 - val_loss: 0.0525 - val_accuracy: 0.9856\n",
            "Epoch 200/400\n",
            "54/54 - 6s - loss: 0.0269 - accuracy: 0.9941 - val_loss: 0.0572 - val_accuracy: 0.9850\n",
            "Epoch 201/400\n",
            "54/54 - 6s - loss: 0.0409 - accuracy: 0.9859 - val_loss: 0.0528 - val_accuracy: 0.9850\n",
            "Epoch 202/400\n",
            "54/54 - 6s - loss: 0.0323 - accuracy: 0.9906 - val_loss: 0.0484 - val_accuracy: 0.9850\n",
            "Epoch 203/400\n",
            "54/54 - 6s - loss: 0.0313 - accuracy: 0.9894 - val_loss: 0.0528 - val_accuracy: 0.9850\n",
            "Epoch 204/400\n",
            "54/54 - 6s - loss: 0.0328 - accuracy: 0.9902 - val_loss: 0.0793 - val_accuracy: 0.9772\n",
            "Epoch 205/400\n",
            "54/54 - 6s - loss: 0.0289 - accuracy: 0.9907 - val_loss: 0.0467 - val_accuracy: 0.9883\n",
            "Epoch 206/400\n",
            "54/54 - 6s - loss: 0.0271 - accuracy: 0.9920 - val_loss: 0.0437 - val_accuracy: 0.9878\n",
            "Epoch 207/400\n",
            "54/54 - 6s - loss: 0.0292 - accuracy: 0.9907 - val_loss: 0.0451 - val_accuracy: 0.9889\n",
            "Epoch 208/400\n",
            "54/54 - 6s - loss: 0.0234 - accuracy: 0.9939 - val_loss: 0.0393 - val_accuracy: 0.9878\n",
            "Epoch 209/400\n",
            "54/54 - 6s - loss: 0.0285 - accuracy: 0.9922 - val_loss: 0.0585 - val_accuracy: 0.9839\n",
            "Epoch 210/400\n",
            "54/54 - 6s - loss: 0.0248 - accuracy: 0.9930 - val_loss: 0.0439 - val_accuracy: 0.9856\n",
            "Epoch 211/400\n",
            "54/54 - 6s - loss: 0.0281 - accuracy: 0.9907 - val_loss: 0.0463 - val_accuracy: 0.9900\n",
            "Epoch 212/400\n",
            "54/54 - 6s - loss: 0.0239 - accuracy: 0.9935 - val_loss: 0.0535 - val_accuracy: 0.9856\n",
            "Epoch 213/400\n",
            "54/54 - 6s - loss: 0.0232 - accuracy: 0.9928 - val_loss: 0.0493 - val_accuracy: 0.9878\n",
            "Epoch 214/400\n",
            "54/54 - 6s - loss: 0.0262 - accuracy: 0.9922 - val_loss: 0.0438 - val_accuracy: 0.9878\n",
            "Epoch 215/400\n",
            "54/54 - 6s - loss: 0.0235 - accuracy: 0.9941 - val_loss: 0.0374 - val_accuracy: 0.9900\n",
            "Epoch 216/400\n",
            "54/54 - 6s - loss: 0.0274 - accuracy: 0.9911 - val_loss: 0.0450 - val_accuracy: 0.9883\n",
            "Epoch 217/400\n",
            "54/54 - 6s - loss: 0.0224 - accuracy: 0.9924 - val_loss: 0.0410 - val_accuracy: 0.9878\n",
            "Epoch 218/400\n",
            "54/54 - 6s - loss: 0.0258 - accuracy: 0.9926 - val_loss: 0.0799 - val_accuracy: 0.9783\n",
            "Epoch 219/400\n",
            "54/54 - 6s - loss: 0.0285 - accuracy: 0.9920 - val_loss: 0.0350 - val_accuracy: 0.9894\n",
            "Epoch 220/400\n",
            "54/54 - 6s - loss: 0.0226 - accuracy: 0.9933 - val_loss: 0.0462 - val_accuracy: 0.9889\n",
            "Epoch 221/400\n",
            "54/54 - 6s - loss: 0.0200 - accuracy: 0.9946 - val_loss: 0.0478 - val_accuracy: 0.9861\n",
            "Epoch 222/400\n",
            "54/54 - 6s - loss: 0.0232 - accuracy: 0.9915 - val_loss: 0.0379 - val_accuracy: 0.9883\n",
            "Epoch 223/400\n",
            "54/54 - 6s - loss: 0.0272 - accuracy: 0.9920 - val_loss: 0.0447 - val_accuracy: 0.9911\n",
            "Epoch 224/400\n",
            "54/54 - 6s - loss: 0.0209 - accuracy: 0.9931 - val_loss: 0.0436 - val_accuracy: 0.9883\n",
            "Epoch 225/400\n",
            "54/54 - 6s - loss: 0.0213 - accuracy: 0.9937 - val_loss: 0.0426 - val_accuracy: 0.9906\n",
            "Epoch 226/400\n",
            "54/54 - 6s - loss: 0.0214 - accuracy: 0.9946 - val_loss: 0.0455 - val_accuracy: 0.9883\n",
            "Epoch 227/400\n",
            "54/54 - 6s - loss: 0.0209 - accuracy: 0.9950 - val_loss: 0.0972 - val_accuracy: 0.9728\n",
            "Epoch 228/400\n",
            "54/54 - 6s - loss: 0.0255 - accuracy: 0.9937 - val_loss: 0.0500 - val_accuracy: 0.9828\n",
            "Epoch 229/400\n",
            "54/54 - 6s - loss: 0.0260 - accuracy: 0.9928 - val_loss: 0.0491 - val_accuracy: 0.9856\n",
            "Epoch 230/400\n",
            "54/54 - 6s - loss: 0.0200 - accuracy: 0.9954 - val_loss: 0.0419 - val_accuracy: 0.9883\n",
            "Epoch 231/400\n",
            "54/54 - 6s - loss: 0.0211 - accuracy: 0.9941 - val_loss: 0.0365 - val_accuracy: 0.9906\n",
            "Epoch 232/400\n",
            "54/54 - 6s - loss: 0.0199 - accuracy: 0.9943 - val_loss: 0.0441 - val_accuracy: 0.9850\n",
            "Epoch 233/400\n",
            "54/54 - 6s - loss: 0.0234 - accuracy: 0.9924 - val_loss: 0.0413 - val_accuracy: 0.9883\n",
            "Epoch 234/400\n",
            "54/54 - 6s - loss: 0.0226 - accuracy: 0.9933 - val_loss: 0.0348 - val_accuracy: 0.9911\n",
            "Epoch 235/400\n",
            "54/54 - 6s - loss: 0.0183 - accuracy: 0.9950 - val_loss: 0.0367 - val_accuracy: 0.9883\n",
            "Epoch 236/400\n",
            "54/54 - 6s - loss: 0.0246 - accuracy: 0.9931 - val_loss: 0.0401 - val_accuracy: 0.9872\n",
            "Epoch 237/400\n",
            "54/54 - 6s - loss: 0.0196 - accuracy: 0.9943 - val_loss: 0.0538 - val_accuracy: 0.9906\n",
            "Epoch 238/400\n",
            "54/54 - 6s - loss: 0.0171 - accuracy: 0.9935 - val_loss: 0.0417 - val_accuracy: 0.9861\n",
            "Epoch 239/400\n",
            "54/54 - 6s - loss: 0.0276 - accuracy: 0.9920 - val_loss: 0.0402 - val_accuracy: 0.9900\n",
            "Epoch 240/400\n",
            "54/54 - 6s - loss: 0.0191 - accuracy: 0.9943 - val_loss: 0.0416 - val_accuracy: 0.9883\n",
            "Epoch 241/400\n",
            "54/54 - 6s - loss: 0.0209 - accuracy: 0.9937 - val_loss: 0.0818 - val_accuracy: 0.9794\n",
            "Epoch 242/400\n",
            "54/54 - 6s - loss: 0.0234 - accuracy: 0.9917 - val_loss: 0.0451 - val_accuracy: 0.9906\n",
            "Epoch 243/400\n",
            "54/54 - 6s - loss: 0.0190 - accuracy: 0.9937 - val_loss: 0.0403 - val_accuracy: 0.9900\n",
            "Epoch 244/400\n",
            "54/54 - 6s - loss: 0.0212 - accuracy: 0.9944 - val_loss: 0.0375 - val_accuracy: 0.9883\n",
            "Epoch 245/400\n",
            "54/54 - 6s - loss: 0.0159 - accuracy: 0.9961 - val_loss: 0.0407 - val_accuracy: 0.9894\n",
            "Epoch 246/400\n",
            "54/54 - 6s - loss: 0.0145 - accuracy: 0.9957 - val_loss: 0.0414 - val_accuracy: 0.9889\n",
            "Epoch 247/400\n",
            "54/54 - 6s - loss: 0.0184 - accuracy: 0.9944 - val_loss: 0.0506 - val_accuracy: 0.9872\n",
            "Epoch 248/400\n",
            "54/54 - 6s - loss: 0.0353 - accuracy: 0.9893 - val_loss: 0.0541 - val_accuracy: 0.9828\n",
            "Epoch 249/400\n",
            "54/54 - 6s - loss: 0.0187 - accuracy: 0.9954 - val_loss: 0.0406 - val_accuracy: 0.9878\n",
            "Epoch 250/400\n",
            "54/54 - 6s - loss: 0.0211 - accuracy: 0.9937 - val_loss: 0.0404 - val_accuracy: 0.9878\n",
            "Epoch 251/400\n",
            "54/54 - 6s - loss: 0.0262 - accuracy: 0.9922 - val_loss: 0.0391 - val_accuracy: 0.9894\n",
            "Epoch 252/400\n",
            "54/54 - 6s - loss: 0.0240 - accuracy: 0.9924 - val_loss: 0.0434 - val_accuracy: 0.9867\n",
            "Epoch 253/400\n",
            "54/54 - 6s - loss: 0.0171 - accuracy: 0.9963 - val_loss: 0.0422 - val_accuracy: 0.9883\n",
            "Epoch 254/400\n",
            "54/54 - 6s - loss: 0.0160 - accuracy: 0.9944 - val_loss: 0.0333 - val_accuracy: 0.9889\n",
            "Epoch 255/400\n",
            "54/54 - 6s - loss: 0.0230 - accuracy: 0.9928 - val_loss: 0.0609 - val_accuracy: 0.9850\n",
            "Epoch 256/400\n",
            "54/54 - 6s - loss: 0.0160 - accuracy: 0.9954 - val_loss: 0.0334 - val_accuracy: 0.9911\n",
            "Epoch 257/400\n",
            "54/54 - 6s - loss: 0.0145 - accuracy: 0.9957 - val_loss: 0.0550 - val_accuracy: 0.9872\n",
            "Epoch 258/400\n",
            "54/54 - 6s - loss: 0.0289 - accuracy: 0.9911 - val_loss: 0.0299 - val_accuracy: 0.9900\n",
            "Epoch 259/400\n",
            "54/54 - 6s - loss: 0.0184 - accuracy: 0.9948 - val_loss: 0.0384 - val_accuracy: 0.9911\n",
            "Epoch 260/400\n",
            "54/54 - 6s - loss: 0.0172 - accuracy: 0.9961 - val_loss: 0.0470 - val_accuracy: 0.9883\n",
            "Epoch 261/400\n",
            "54/54 - 6s - loss: 0.0170 - accuracy: 0.9957 - val_loss: 0.0538 - val_accuracy: 0.9872\n",
            "Epoch 262/400\n",
            "54/54 - 6s - loss: 0.0177 - accuracy: 0.9952 - val_loss: 0.0336 - val_accuracy: 0.9917\n",
            "Epoch 263/400\n",
            "54/54 - 6s - loss: 0.0134 - accuracy: 0.9963 - val_loss: 0.0374 - val_accuracy: 0.9911\n",
            "Epoch 264/400\n",
            "54/54 - 6s - loss: 0.0129 - accuracy: 0.9961 - val_loss: 0.0442 - val_accuracy: 0.9856\n",
            "Epoch 265/400\n",
            "54/54 - 6s - loss: 0.0174 - accuracy: 0.9957 - val_loss: 0.0536 - val_accuracy: 0.9861\n",
            "Epoch 266/400\n",
            "54/54 - 6s - loss: 0.0247 - accuracy: 0.9915 - val_loss: 0.0499 - val_accuracy: 0.9872\n",
            "Epoch 267/400\n",
            "54/54 - 6s - loss: 0.0134 - accuracy: 0.9961 - val_loss: 0.0473 - val_accuracy: 0.9844\n",
            "Epoch 268/400\n",
            "54/54 - 6s - loss: 0.0146 - accuracy: 0.9963 - val_loss: 0.0583 - val_accuracy: 0.9856\n",
            "Epoch 269/400\n",
            "54/54 - 6s - loss: 0.0188 - accuracy: 0.9943 - val_loss: 0.0310 - val_accuracy: 0.9917\n",
            "Epoch 270/400\n",
            "54/54 - 6s - loss: 0.0100 - accuracy: 0.9980 - val_loss: 0.0416 - val_accuracy: 0.9867\n",
            "Epoch 271/400\n",
            "54/54 - 6s - loss: 0.0116 - accuracy: 0.9967 - val_loss: 0.0439 - val_accuracy: 0.9889\n",
            "Epoch 272/400\n",
            "54/54 - 6s - loss: 0.0163 - accuracy: 0.9959 - val_loss: 0.0434 - val_accuracy: 0.9861\n",
            "Epoch 273/400\n",
            "54/54 - 6s - loss: 0.0185 - accuracy: 0.9948 - val_loss: 0.0491 - val_accuracy: 0.9911\n",
            "Epoch 274/400\n",
            "54/54 - 6s - loss: 0.0124 - accuracy: 0.9961 - val_loss: 0.0432 - val_accuracy: 0.9889\n",
            "Epoch 275/400\n",
            "54/54 - 6s - loss: 0.0157 - accuracy: 0.9956 - val_loss: 0.0280 - val_accuracy: 0.9894\n",
            "Epoch 276/400\n",
            "54/54 - 6s - loss: 0.0192 - accuracy: 0.9950 - val_loss: 0.0278 - val_accuracy: 0.9933\n",
            "Epoch 277/400\n",
            "54/54 - 6s - loss: 0.0081 - accuracy: 0.9985 - val_loss: 0.0551 - val_accuracy: 0.9867\n",
            "Epoch 278/400\n",
            "54/54 - 6s - loss: 0.0111 - accuracy: 0.9970 - val_loss: 0.0367 - val_accuracy: 0.9906\n",
            "Epoch 279/400\n",
            "54/54 - 6s - loss: 0.0092 - accuracy: 0.9976 - val_loss: 0.0323 - val_accuracy: 0.9894\n",
            "Epoch 280/400\n",
            "54/54 - 6s - loss: 0.0115 - accuracy: 0.9970 - val_loss: 0.0339 - val_accuracy: 0.9894\n",
            "Epoch 281/400\n",
            "54/54 - 6s - loss: 0.0131 - accuracy: 0.9965 - val_loss: 0.0331 - val_accuracy: 0.9900\n",
            "Epoch 282/400\n",
            "54/54 - 6s - loss: 0.0098 - accuracy: 0.9969 - val_loss: 0.0470 - val_accuracy: 0.9911\n",
            "Epoch 283/400\n",
            "54/54 - 6s - loss: 0.0118 - accuracy: 0.9967 - val_loss: 0.0491 - val_accuracy: 0.9894\n",
            "Epoch 284/400\n",
            "54/54 - 6s - loss: 0.0103 - accuracy: 0.9978 - val_loss: 0.0556 - val_accuracy: 0.9872\n",
            "Epoch 285/400\n",
            "54/54 - 6s - loss: 0.0121 - accuracy: 0.9967 - val_loss: 0.0316 - val_accuracy: 0.9922\n",
            "Epoch 286/400\n",
            "54/54 - 6s - loss: 0.0113 - accuracy: 0.9969 - val_loss: 0.0420 - val_accuracy: 0.9872\n",
            "Epoch 287/400\n",
            "54/54 - 6s - loss: 0.0160 - accuracy: 0.9952 - val_loss: 0.0465 - val_accuracy: 0.9867\n",
            "Epoch 288/400\n",
            "54/54 - 6s - loss: 0.0136 - accuracy: 0.9959 - val_loss: 0.0419 - val_accuracy: 0.9894\n",
            "Epoch 289/400\n",
            "54/54 - 6s - loss: 0.0165 - accuracy: 0.9957 - val_loss: 0.0515 - val_accuracy: 0.9883\n",
            "Epoch 290/400\n",
            "54/54 - 6s - loss: 0.0135 - accuracy: 0.9954 - val_loss: 0.0403 - val_accuracy: 0.9878\n",
            "Epoch 291/400\n",
            "54/54 - 6s - loss: 0.0191 - accuracy: 0.9944 - val_loss: 0.0604 - val_accuracy: 0.9817\n",
            "Epoch 292/400\n",
            "54/54 - 6s - loss: 0.0132 - accuracy: 0.9959 - val_loss: 0.0339 - val_accuracy: 0.9878\n",
            "Epoch 293/400\n",
            "54/54 - 6s - loss: 0.0191 - accuracy: 0.9939 - val_loss: 0.0522 - val_accuracy: 0.9894\n",
            "Epoch 294/400\n",
            "54/54 - 6s - loss: 0.0098 - accuracy: 0.9970 - val_loss: 0.0343 - val_accuracy: 0.9906\n",
            "Epoch 295/400\n",
            "54/54 - 6s - loss: 0.0202 - accuracy: 0.9943 - val_loss: 0.0369 - val_accuracy: 0.9883\n",
            "Epoch 296/400\n",
            "54/54 - 6s - loss: 0.0121 - accuracy: 0.9969 - val_loss: 0.0288 - val_accuracy: 0.9922\n",
            "Epoch 297/400\n",
            "54/54 - 6s - loss: 0.0161 - accuracy: 0.9957 - val_loss: 0.0295 - val_accuracy: 0.9911\n",
            "Epoch 298/400\n",
            "54/54 - 6s - loss: 0.0089 - accuracy: 0.9980 - val_loss: 0.0439 - val_accuracy: 0.9878\n",
            "Epoch 299/400\n",
            "54/54 - 6s - loss: 0.0087 - accuracy: 0.9972 - val_loss: 0.0464 - val_accuracy: 0.9844\n",
            "Epoch 300/400\n",
            "54/54 - 6s - loss: 0.0096 - accuracy: 0.9972 - val_loss: 0.0509 - val_accuracy: 0.9872\n",
            "Epoch 301/400\n",
            "54/54 - 6s - loss: 0.0114 - accuracy: 0.9963 - val_loss: 0.0289 - val_accuracy: 0.9922\n",
            "Epoch 302/400\n",
            "54/54 - 6s - loss: 0.0184 - accuracy: 0.9948 - val_loss: 0.0400 - val_accuracy: 0.9867\n",
            "Epoch 303/400\n",
            "54/54 - 6s - loss: 0.0082 - accuracy: 0.9978 - val_loss: 0.0545 - val_accuracy: 0.9894\n",
            "Epoch 304/400\n",
            "54/54 - 6s - loss: 0.0091 - accuracy: 0.9980 - val_loss: 0.0441 - val_accuracy: 0.9883\n",
            "Epoch 305/400\n",
            "54/54 - 6s - loss: 0.0090 - accuracy: 0.9970 - val_loss: 0.0346 - val_accuracy: 0.9911\n",
            "Epoch 306/400\n",
            "54/54 - 6s - loss: 0.0119 - accuracy: 0.9969 - val_loss: 0.0373 - val_accuracy: 0.9900\n",
            "Epoch 307/400\n",
            "54/54 - 6s - loss: 0.0082 - accuracy: 0.9974 - val_loss: 0.0396 - val_accuracy: 0.9900\n",
            "Epoch 308/400\n",
            "54/54 - 6s - loss: 0.0096 - accuracy: 0.9976 - val_loss: 0.0394 - val_accuracy: 0.9911\n",
            "Epoch 309/400\n",
            "54/54 - 6s - loss: 0.0096 - accuracy: 0.9976 - val_loss: 0.0521 - val_accuracy: 0.9878\n",
            "Epoch 310/400\n",
            "54/54 - 6s - loss: 0.0127 - accuracy: 0.9967 - val_loss: 0.0719 - val_accuracy: 0.9806\n",
            "Epoch 311/400\n",
            "54/54 - 6s - loss: 0.0087 - accuracy: 0.9980 - val_loss: 0.0563 - val_accuracy: 0.9894\n",
            "Epoch 312/400\n",
            "54/54 - 6s - loss: 0.0081 - accuracy: 0.9980 - val_loss: 0.0255 - val_accuracy: 0.9933\n",
            "Epoch 313/400\n",
            "54/54 - 6s - loss: 0.0146 - accuracy: 0.9954 - val_loss: 0.2802 - val_accuracy: 0.9439\n",
            "Epoch 314/400\n",
            "54/54 - 6s - loss: 0.0257 - accuracy: 0.9922 - val_loss: 0.0291 - val_accuracy: 0.9911\n",
            "Epoch 315/400\n",
            "54/54 - 6s - loss: 0.0110 - accuracy: 0.9961 - val_loss: 0.0252 - val_accuracy: 0.9928\n",
            "Epoch 316/400\n",
            "54/54 - 6s - loss: 0.0118 - accuracy: 0.9967 - val_loss: 0.0357 - val_accuracy: 0.9911\n",
            "Epoch 317/400\n",
            "54/54 - 6s - loss: 0.0109 - accuracy: 0.9976 - val_loss: 0.0479 - val_accuracy: 0.9917\n",
            "Epoch 318/400\n",
            "54/54 - 6s - loss: 0.0109 - accuracy: 0.9972 - val_loss: 0.0290 - val_accuracy: 0.9878\n",
            "Epoch 319/400\n",
            "54/54 - 6s - loss: 0.0146 - accuracy: 0.9946 - val_loss: 0.0254 - val_accuracy: 0.9922\n",
            "Epoch 320/400\n",
            "54/54 - 6s - loss: 0.0109 - accuracy: 0.9967 - val_loss: 0.0426 - val_accuracy: 0.9911\n",
            "Epoch 321/400\n",
            "54/54 - 6s - loss: 0.0089 - accuracy: 0.9983 - val_loss: 0.0656 - val_accuracy: 0.9811\n",
            "Epoch 322/400\n",
            "54/54 - 6s - loss: 0.0143 - accuracy: 0.9952 - val_loss: 0.0695 - val_accuracy: 0.9822\n",
            "Epoch 323/400\n",
            "54/54 - 6s - loss: 0.0266 - accuracy: 0.9917 - val_loss: 0.0331 - val_accuracy: 0.9928\n",
            "Epoch 324/400\n",
            "54/54 - 6s - loss: 0.0076 - accuracy: 0.9983 - val_loss: 0.0358 - val_accuracy: 0.9883\n",
            "Epoch 325/400\n",
            "54/54 - 6s - loss: 0.0059 - accuracy: 0.9987 - val_loss: 0.0439 - val_accuracy: 0.9900\n",
            "Epoch 326/400\n",
            "54/54 - 6s - loss: 0.0084 - accuracy: 0.9972 - val_loss: 0.0558 - val_accuracy: 0.9850\n",
            "Epoch 327/400\n",
            "54/54 - 6s - loss: 0.0073 - accuracy: 0.9981 - val_loss: 0.0683 - val_accuracy: 0.9850\n",
            "Epoch 328/400\n",
            "54/54 - 6s - loss: 0.0076 - accuracy: 0.9980 - val_loss: 0.0250 - val_accuracy: 0.9928\n",
            "Epoch 329/400\n",
            "54/54 - 6s - loss: 0.0087 - accuracy: 0.9972 - val_loss: 0.0585 - val_accuracy: 0.9839\n",
            "Epoch 330/400\n",
            "54/54 - 6s - loss: 0.0090 - accuracy: 0.9978 - val_loss: 0.0351 - val_accuracy: 0.9894\n",
            "Epoch 331/400\n",
            "54/54 - 6s - loss: 0.0075 - accuracy: 0.9978 - val_loss: 0.0414 - val_accuracy: 0.9922\n",
            "Epoch 332/400\n",
            "54/54 - 6s - loss: 0.0083 - accuracy: 0.9980 - val_loss: 0.0371 - val_accuracy: 0.9894\n",
            "Epoch 333/400\n",
            "54/54 - 6s - loss: 0.0100 - accuracy: 0.9969 - val_loss: 0.0667 - val_accuracy: 0.9844\n",
            "Epoch 334/400\n",
            "54/54 - 6s - loss: 0.0101 - accuracy: 0.9972 - val_loss: 0.0404 - val_accuracy: 0.9906\n",
            "Epoch 335/400\n",
            "54/54 - 6s - loss: 0.0091 - accuracy: 0.9978 - val_loss: 0.0566 - val_accuracy: 0.9900\n",
            "Epoch 336/400\n",
            "54/54 - 6s - loss: 0.0128 - accuracy: 0.9965 - val_loss: 0.0281 - val_accuracy: 0.9889\n",
            "Epoch 337/400\n",
            "54/54 - 6s - loss: 0.0076 - accuracy: 0.9978 - val_loss: 0.0257 - val_accuracy: 0.9933\n",
            "Epoch 338/400\n",
            "54/54 - 6s - loss: 0.0069 - accuracy: 0.9987 - val_loss: 0.0253 - val_accuracy: 0.9917\n",
            "Epoch 339/400\n",
            "54/54 - 6s - loss: 0.0098 - accuracy: 0.9976 - val_loss: 0.0261 - val_accuracy: 0.9917\n",
            "Epoch 340/400\n",
            "54/54 - 6s - loss: 0.0133 - accuracy: 0.9957 - val_loss: 0.0235 - val_accuracy: 0.9928\n",
            "Epoch 341/400\n",
            "54/54 - 6s - loss: 0.0143 - accuracy: 0.9954 - val_loss: 0.1720 - val_accuracy: 0.9167\n",
            "Epoch 342/400\n",
            "54/54 - 6s - loss: 0.0246 - accuracy: 0.9920 - val_loss: 0.0324 - val_accuracy: 0.9917\n",
            "Epoch 343/400\n",
            "54/54 - 6s - loss: 0.0103 - accuracy: 0.9972 - val_loss: 0.0320 - val_accuracy: 0.9894\n",
            "Epoch 344/400\n",
            "54/54 - 6s - loss: 0.0061 - accuracy: 0.9985 - val_loss: 0.0447 - val_accuracy: 0.9894\n",
            "Epoch 345/400\n",
            "54/54 - 6s - loss: 0.0090 - accuracy: 0.9981 - val_loss: 0.0512 - val_accuracy: 0.9883\n",
            "Epoch 346/400\n",
            "54/54 - 6s - loss: 0.0144 - accuracy: 0.9956 - val_loss: 0.0394 - val_accuracy: 0.9906\n",
            "Epoch 347/400\n",
            "54/54 - 6s - loss: 0.0101 - accuracy: 0.9970 - val_loss: 0.0378 - val_accuracy: 0.9889\n",
            "Epoch 348/400\n",
            "54/54 - 6s - loss: 0.0079 - accuracy: 0.9983 - val_loss: 0.0301 - val_accuracy: 0.9917\n",
            "Epoch 349/400\n",
            "54/54 - 6s - loss: 0.0058 - accuracy: 0.9985 - val_loss: 0.0285 - val_accuracy: 0.9922\n",
            "Epoch 350/400\n",
            "54/54 - 6s - loss: 0.0058 - accuracy: 0.9981 - val_loss: 0.0431 - val_accuracy: 0.9900\n",
            "Epoch 351/400\n",
            "54/54 - 6s - loss: 0.0056 - accuracy: 0.9987 - val_loss: 0.0308 - val_accuracy: 0.9922\n",
            "Epoch 352/400\n",
            "54/54 - 6s - loss: 0.0125 - accuracy: 0.9965 - val_loss: 0.0261 - val_accuracy: 0.9911\n",
            "Epoch 353/400\n",
            "54/54 - 6s - loss: 0.0091 - accuracy: 0.9976 - val_loss: 0.0447 - val_accuracy: 0.9900\n",
            "Epoch 354/400\n",
            "54/54 - 6s - loss: 0.0099 - accuracy: 0.9972 - val_loss: 0.0395 - val_accuracy: 0.9911\n",
            "Epoch 355/400\n",
            "54/54 - 6s - loss: 0.0074 - accuracy: 0.9980 - val_loss: 0.0296 - val_accuracy: 0.9928\n",
            "Epoch 356/400\n",
            "54/54 - 6s - loss: 0.0111 - accuracy: 0.9967 - val_loss: 0.0831 - val_accuracy: 0.9822\n",
            "Epoch 357/400\n",
            "54/54 - 6s - loss: 0.0075 - accuracy: 0.9978 - val_loss: 0.0475 - val_accuracy: 0.9894\n",
            "Epoch 358/400\n",
            "54/54 - 6s - loss: 0.0065 - accuracy: 0.9981 - val_loss: 0.0314 - val_accuracy: 0.9922\n",
            "Epoch 359/400\n",
            "54/54 - 6s - loss: 0.0082 - accuracy: 0.9980 - val_loss: 0.0516 - val_accuracy: 0.9878\n",
            "Epoch 360/400\n",
            "54/54 - 6s - loss: 0.0078 - accuracy: 0.9978 - val_loss: 0.0294 - val_accuracy: 0.9922\n",
            "Epoch 361/400\n",
            "54/54 - 6s - loss: 0.0145 - accuracy: 0.9944 - val_loss: 0.0374 - val_accuracy: 0.9922\n",
            "Epoch 362/400\n",
            "54/54 - 6s - loss: 0.0069 - accuracy: 0.9981 - val_loss: 0.0532 - val_accuracy: 0.9906\n",
            "Epoch 363/400\n",
            "54/54 - 6s - loss: 0.0065 - accuracy: 0.9983 - val_loss: 0.0286 - val_accuracy: 0.9922\n",
            "Epoch 364/400\n",
            "54/54 - 6s - loss: 0.0071 - accuracy: 0.9981 - val_loss: 0.0788 - val_accuracy: 0.9800\n",
            "Epoch 365/400\n",
            "54/54 - 6s - loss: 0.0103 - accuracy: 0.9980 - val_loss: 0.0489 - val_accuracy: 0.9889\n",
            "Epoch 366/400\n",
            "54/54 - 6s - loss: 0.0160 - accuracy: 0.9954 - val_loss: 0.0464 - val_accuracy: 0.9861\n",
            "Epoch 367/400\n",
            "54/54 - 6s - loss: 0.0111 - accuracy: 0.9970 - val_loss: 0.0482 - val_accuracy: 0.9861\n",
            "Epoch 368/400\n",
            "54/54 - 6s - loss: 0.0074 - accuracy: 0.9985 - val_loss: 0.0385 - val_accuracy: 0.9906\n",
            "Epoch 369/400\n",
            "54/54 - 6s - loss: 0.0044 - accuracy: 0.9989 - val_loss: 0.0425 - val_accuracy: 0.9911\n",
            "Epoch 370/400\n",
            "54/54 - 6s - loss: 0.0087 - accuracy: 0.9976 - val_loss: 0.0405 - val_accuracy: 0.9906\n",
            "Epoch 371/400\n",
            "54/54 - 6s - loss: 0.0089 - accuracy: 0.9980 - val_loss: 0.0426 - val_accuracy: 0.9933\n",
            "Epoch 372/400\n",
            "54/54 - 6s - loss: 0.0071 - accuracy: 0.9985 - val_loss: 0.0510 - val_accuracy: 0.9889\n",
            "Epoch 373/400\n",
            "54/54 - 6s - loss: 0.0077 - accuracy: 0.9980 - val_loss: 0.0247 - val_accuracy: 0.9928\n",
            "Epoch 374/400\n",
            "54/54 - 6s - loss: 0.0056 - accuracy: 0.9991 - val_loss: 0.0361 - val_accuracy: 0.9894\n",
            "Epoch 375/400\n",
            "54/54 - 6s - loss: 0.0049 - accuracy: 0.9985 - val_loss: 0.0525 - val_accuracy: 0.9867\n",
            "Epoch 376/400\n",
            "54/54 - 6s - loss: 0.0093 - accuracy: 0.9967 - val_loss: 0.0359 - val_accuracy: 0.9883\n",
            "Epoch 377/400\n",
            "54/54 - 6s - loss: 0.0096 - accuracy: 0.9974 - val_loss: 0.0527 - val_accuracy: 0.9894\n",
            "Epoch 378/400\n",
            "54/54 - 6s - loss: 0.0051 - accuracy: 0.9989 - val_loss: 0.0465 - val_accuracy: 0.9894\n",
            "Epoch 379/400\n",
            "54/54 - 6s - loss: 0.0159 - accuracy: 0.9954 - val_loss: 0.0358 - val_accuracy: 0.9906\n",
            "Epoch 380/400\n",
            "54/54 - 6s - loss: 0.0077 - accuracy: 0.9978 - val_loss: 0.0290 - val_accuracy: 0.9928\n",
            "Epoch 381/400\n",
            "54/54 - 6s - loss: 0.0045 - accuracy: 0.9987 - val_loss: 0.0317 - val_accuracy: 0.9900\n",
            "Epoch 382/400\n",
            "54/54 - 6s - loss: 0.0067 - accuracy: 0.9981 - val_loss: 0.0253 - val_accuracy: 0.9917\n",
            "Epoch 383/400\n",
            "54/54 - 6s - loss: 0.0111 - accuracy: 0.9961 - val_loss: 0.1147 - val_accuracy: 0.9706\n",
            "Epoch 384/400\n",
            "54/54 - 6s - loss: 0.0173 - accuracy: 0.9944 - val_loss: 0.0918 - val_accuracy: 0.9739\n",
            "Epoch 385/400\n",
            "54/54 - 6s - loss: 0.0085 - accuracy: 0.9983 - val_loss: 0.0797 - val_accuracy: 0.9789\n",
            "Epoch 386/400\n",
            "54/54 - 6s - loss: 0.0117 - accuracy: 0.9957 - val_loss: 0.0303 - val_accuracy: 0.9906\n",
            "Epoch 387/400\n",
            "54/54 - 6s - loss: 0.0116 - accuracy: 0.9959 - val_loss: 0.0325 - val_accuracy: 0.9900\n",
            "Epoch 388/400\n",
            "54/54 - 6s - loss: 0.0071 - accuracy: 0.9983 - val_loss: 0.0322 - val_accuracy: 0.9906\n",
            "Epoch 389/400\n",
            "54/54 - 6s - loss: 0.0062 - accuracy: 0.9983 - val_loss: 0.0714 - val_accuracy: 0.9833\n",
            "Epoch 390/400\n",
            "54/54 - 6s - loss: 0.0068 - accuracy: 0.9987 - val_loss: 0.0400 - val_accuracy: 0.9906\n",
            "Epoch 391/400\n",
            "54/54 - 6s - loss: 0.0049 - accuracy: 0.9989 - val_loss: 0.0376 - val_accuracy: 0.9922\n",
            "Epoch 392/400\n",
            "54/54 - 6s - loss: 0.0060 - accuracy: 0.9985 - val_loss: 0.0331 - val_accuracy: 0.9900\n",
            "Epoch 393/400\n",
            "54/54 - 6s - loss: 0.0082 - accuracy: 0.9976 - val_loss: 0.0283 - val_accuracy: 0.9928\n",
            "Epoch 394/400\n",
            "54/54 - 6s - loss: 0.0276 - accuracy: 0.9894 - val_loss: 0.0442 - val_accuracy: 0.9844\n",
            "Epoch 395/400\n",
            "54/54 - 6s - loss: 0.0200 - accuracy: 0.9928 - val_loss: 0.0308 - val_accuracy: 0.9917\n",
            "Epoch 396/400\n",
            "54/54 - 6s - loss: 0.0070 - accuracy: 0.9985 - val_loss: 0.0572 - val_accuracy: 0.9878\n",
            "Epoch 397/400\n",
            "54/54 - 6s - loss: 0.0043 - accuracy: 0.9989 - val_loss: 0.0333 - val_accuracy: 0.9928\n",
            "Epoch 398/400\n",
            "54/54 - 6s - loss: 0.0046 - accuracy: 0.9987 - val_loss: 0.0551 - val_accuracy: 0.9911\n",
            "Epoch 399/400\n",
            "54/54 - 6s - loss: 0.0070 - accuracy: 0.9980 - val_loss: 0.0769 - val_accuracy: 0.9833\n",
            "Epoch 400/400\n",
            "54/54 - 6s - loss: 0.0045 - accuracy: 0.9991 - val_loss: 0.0362 - val_accuracy: 0.9906\n",
            "{'loss': [1.1042592525482178, 1.0201929807662964, 0.8847047686576843, 0.7826791405677795, 0.7374959588050842, 0.7113175988197327, 0.7012317776679993, 0.6793611645698547, 0.6673768758773804, 0.6613796353340149, 0.6481851935386658, 0.6414291858673096, 0.6302599310874939, 0.6239174604415894, 0.6128011345863342, 0.6047210693359375, 0.6100921630859375, 0.598039448261261, 0.5894721150398254, 0.5801272988319397, 0.5793696045875549, 0.5693145990371704, 0.561244785785675, 0.5527955293655396, 0.5441912412643433, 0.5371090173721313, 0.5313543081283569, 0.5232850909233093, 0.5111651420593262, 0.5014651417732239, 0.49854776263237, 0.4876738488674164, 0.4866965413093567, 0.47785142064094543, 0.47116437554359436, 0.45820677280426025, 0.44979262351989746, 0.45544981956481934, 0.43088284134864807, 0.42258626222610474, 0.41012048721313477, 0.4046299457550049, 0.39083221554756165, 0.3921898603439331, 0.3756679594516754, 0.3616238832473755, 0.35117706656455994, 0.3571735620498657, 0.3366410434246063, 0.3265778124332428, 0.3221742510795593, 0.3087156414985657, 0.3003450036048889, 0.29556193947792053, 0.28895583748817444, 0.277132511138916, 0.26593589782714844, 0.26636797189712524, 0.2599909007549286, 0.2592827379703522, 0.24639639258384705, 0.2484559416770935, 0.23710045218467712, 0.22314521670341492, 0.21979987621307373, 0.22038117051124573, 0.212625190615654, 0.20879729092121124, 0.20902769267559052, 0.20563793182373047, 0.19230987131595612, 0.18315193057060242, 0.1866845190525055, 0.1774183213710785, 0.1761065572500229, 0.17123211920261383, 0.17280705273151398, 0.17065486311912537, 0.16639086604118347, 0.17382198572158813, 0.15468721091747284, 0.16643807291984558, 0.14951340854167938, 0.1480482816696167, 0.14747831225395203, 0.15429723262786865, 0.1520044058561325, 0.1374722719192505, 0.13093048334121704, 0.13808704912662506, 0.14086364209651947, 0.12753073871135712, 0.12843407690525055, 0.12428874522447586, 0.13701094686985016, 0.12804535031318665, 0.12061779201030731, 0.12096702307462692, 0.11522667855024338, 0.1149221807718277, 0.11252764612436295, 0.11129076778888702, 0.10630365461111069, 0.10602663457393646, 0.12081003934144974, 0.10406985878944397, 0.10642994195222855, 0.10178209096193314, 0.10923115909099579, 0.09878586232662201, 0.10342711210250854, 0.09091859310865402, 0.1009286418557167, 0.09705593436956406, 0.09536443650722504, 0.0980890765786171, 0.09374317526817322, 0.09669619053602219, 0.08954039216041565, 0.09143675118684769, 0.09382384270429611, 0.08522069454193115, 0.09108789265155792, 0.08986358344554901, 0.08124373853206635, 0.08312895148992538, 0.08271102607250214, 0.08068563044071198, 0.0781034380197525, 0.08379952609539032, 0.07402472198009491, 0.07392878085374832, 0.06868472695350647, 0.08457136154174805, 0.07478485256433487, 0.06559015065431595, 0.07076355814933777, 0.07102040201425552, 0.07128582149744034, 0.06710147112607956, 0.06214156374335289, 0.06410405784845352, 0.07055439800024033, 0.06777966022491455, 0.06663620471954346, 0.06135902181267738, 0.05421743169426918, 0.06691546738147736, 0.06220623850822449, 0.06289754062891006, 0.05931157246232033, 0.05355650559067726, 0.05503669008612633, 0.04973438009619713, 0.0592600516974926, 0.050827570259571075, 0.05168876424431801, 0.05811909958720207, 0.052171748131513596, 0.05548669397830963, 0.05724890157580376, 0.056497614830732346, 0.04524802416563034, 0.05591559037566185, 0.04479651153087616, 0.04990285634994507, 0.04781263694167137, 0.04550942778587341, 0.04825138673186302, 0.04980386421084404, 0.04344167187809944, 0.047273196280002594, 0.04557212442159653, 0.04532873257994652, 0.03862965479493141, 0.05645535886287689, 0.04324192553758621, 0.041258860379457474, 0.0390268936753273, 0.04573368281126022, 0.03859638050198555, 0.038442209362983704, 0.03583936393260956, 0.04334884509444237, 0.04092009365558624, 0.037704404443502426, 0.03267749771475792, 0.03787703439593315, 0.03483094274997711, 0.03289828076958656, 0.03456388786435127, 0.046943724155426025, 0.03701961785554886, 0.03322984278202057, 0.0365060493350029, 0.03199310973286629, 0.03265383467078209, 0.03506855294108391, 0.03220845386385918, 0.026932287961244583, 0.04092059284448624, 0.032273948192596436, 0.031337153166532516, 0.032799676060676575, 0.028872745111584663, 0.027107970789074898, 0.02923043631017208, 0.023386294022202492, 0.028457138687372208, 0.02475646696984768, 0.028129134327173233, 0.02386242337524891, 0.02320037968456745, 0.026180600747466087, 0.023464150726795197, 0.02735859341919422, 0.02239781618118286, 0.025833874940872192, 0.028523394837975502, 0.02256503701210022, 0.019973870366811752, 0.0231629665941, 0.02722957171499729, 0.020894881337881088, 0.0213229488581419, 0.021422704681754112, 0.02094527892768383, 0.025491004809737206, 0.026028843596577644, 0.019980357959866524, 0.021101607009768486, 0.019906120374798775, 0.023403406143188477, 0.022647568956017494, 0.018314089626073837, 0.024586526677012444, 0.019600199535489082, 0.017108863219618797, 0.027583153918385506, 0.019095106050372124, 0.020941710099577904, 0.02337031438946724, 0.01896071247756481, 0.021240457892417908, 0.015940574929118156, 0.014544312842190266, 0.01839044876396656, 0.03530061990022659, 0.018745090812444687, 0.021078545600175858, 0.026181120425462723, 0.023961765691637993, 0.01705724000930786, 0.016017144545912743, 0.022985072806477547, 0.01596948318183422, 0.014536604285240173, 0.028891129419207573, 0.018438441678881645, 0.017153939232230186, 0.016976630315184593, 0.017698436975479126, 0.013433057814836502, 0.012883847579360008, 0.01737881638109684, 0.02473222278058529, 0.013426698744297028, 0.01455237902700901, 0.01884138584136963, 0.010028356686234474, 0.011611152440309525, 0.016256382688879967, 0.018496721982955933, 0.012417636811733246, 0.01566663384437561, 0.01919698715209961, 0.00810124259442091, 0.011124501004815102, 0.009163477458059788, 0.01148353610187769, 0.01311403326690197, 0.009815886616706848, 0.01180216297507286, 0.010259561240673065, 0.01208015438169241, 0.011347956955432892, 0.016039635986089706, 0.013579054735600948, 0.016518574208021164, 0.013451776467263699, 0.01905847154557705, 0.013193720020353794, 0.01912401244044304, 0.009833739139139652, 0.020234528928995132, 0.012134685181081295, 0.016133427619934082, 0.00893948134034872, 0.008673829026520252, 0.009607960470020771, 0.011421559378504753, 0.018422534689307213, 0.008188934996724129, 0.009147721342742443, 0.008979243226349354, 0.011879554949700832, 0.008230102248489857, 0.00958893820643425, 0.009629734791815281, 0.012708159163594246, 0.008701994083821774, 0.00808240007609129, 0.014585672877728939, 0.025709621608257294, 0.010963140986859798, 0.01184821780771017, 0.010890508070588112, 0.010940063744783401, 0.014570768922567368, 0.010855816304683685, 0.008881034329533577, 0.014271240681409836, 0.026554321870207787, 0.007566073909401894, 0.005874808877706528, 0.008419569581747055, 0.007321024313569069, 0.007574189454317093, 0.0086623914539814, 0.009016000665724277, 0.007512254174798727, 0.008327996358275414, 0.009969566948711872, 0.010141377337276936, 0.009108197875320911, 0.012795292772352695, 0.007624524645507336, 0.006887243594974279, 0.009793952107429504, 0.013252121396362782, 0.014345346949994564, 0.024620352312922478, 0.010329945012927055, 0.006131167057901621, 0.009008221328258514, 0.014395014382898808, 0.010130545124411583, 0.007901208475232124, 0.005785275250673294, 0.005803989712148905, 0.005560887511819601, 0.012492344714701176, 0.009072343818843365, 0.009916304610669613, 0.007386529818177223, 0.011105423793196678, 0.007528962101787329, 0.006533768959343433, 0.008182368241250515, 0.007790152449160814, 0.014510422945022583, 0.006882471032440662, 0.006542799063026905, 0.007096436340361834, 0.01030612364411354, 0.015962323173880577, 0.011103644035756588, 0.007448635995388031, 0.004388064611703157, 0.008718637749552727, 0.008863315917551517, 0.007052237167954445, 0.007693206425756216, 0.0056434511207044125, 0.004926792811602354, 0.009255639277398586, 0.0095523027703166, 0.005124391056597233, 0.015922384336590767, 0.007695335894823074, 0.004483337979763746, 0.006665138527750969, 0.011095778085291386, 0.017253296449780464, 0.008527958765625954, 0.011743913404643536, 0.011601999402046204, 0.007107173558324575, 0.006231858395040035, 0.006801531184464693, 0.004871519282460213, 0.006026210263371468, 0.008227657526731491, 0.02761557139456272, 0.020018985494971275, 0.006969505455344915, 0.004303556866943836, 0.004612619522958994, 0.00699868006631732, 0.004492048639804125], 'accuracy': [0.3677777647972107, 0.48722222447395325, 0.5651851892471313, 0.6131481528282166, 0.6390740871429443, 0.6527777910232544, 0.6638888716697693, 0.6846296191215515, 0.6901851892471313, 0.693148136138916, 0.7075926065444946, 0.713518500328064, 0.7146296501159668, 0.7225925922393799, 0.7301852107048035, 0.737407386302948, 0.7283333539962769, 0.7368518710136414, 0.7446296215057373, 0.747592568397522, 0.75, 0.7564814686775208, 0.7570370435714722, 0.7633333206176758, 0.7644444704055786, 0.7662962675094604, 0.7716666460037231, 0.7764815092086792, 0.7848148345947266, 0.7827777862548828, 0.7901852130889893, 0.7946296334266663, 0.7900000214576721, 0.8050000071525574, 0.7994444370269775, 0.8122222423553467, 0.8155555725097656, 0.8120370507240295, 0.8140740990638733, 0.8274074196815491, 0.8348147869110107, 0.8329629898071289, 0.837592601776123, 0.8381481766700745, 0.8457407355308533, 0.8596296310424805, 0.857962965965271, 0.851111114025116, 0.862407386302948, 0.8733333349227905, 0.8766666650772095, 0.8820370435714722, 0.8805555701255798, 0.8848147988319397, 0.8929629921913147, 0.8924074172973633, 0.9049999713897705, 0.8972222208976746, 0.9040740728378296, 0.9018518328666687, 0.9062963128089905, 0.9087036848068237, 0.9083333611488342, 0.9153703451156616, 0.9157407283782959, 0.9164814949035645, 0.9233333468437195, 0.9253703951835632, 0.92166668176651, 0.9274073839187622, 0.9327777624130249, 0.9372222423553467, 0.9292592406272888, 0.9338889122009277, 0.9383333325386047, 0.9418518543243408, 0.936296284198761, 0.9398148059844971, 0.9414814710617065, 0.9372222423553467, 0.9449999928474426, 0.9438889026641846, 0.9483333230018616, 0.946481466293335, 0.9485185146331787, 0.9483333230018616, 0.9487037062644958, 0.9529629349708557, 0.9596296548843384, 0.9512962698936462, 0.9509259462356567, 0.9572222232818604, 0.9592592716217041, 0.9598147869110107, 0.9527778029441833, 0.9574074149131775, 0.9596296548843384, 0.9596296548843384, 0.959074079990387, 0.9605555534362793, 0.9624074101448059, 0.9651851654052734, 0.962592601776123, 0.9672222137451172, 0.9585185050964355, 0.9646296501159668, 0.9662963151931763, 0.9657407402992249, 0.9648148417472839, 0.9683333039283752, 0.9664815068244934, 0.9724074006080627, 0.9672222137451172, 0.970370352268219, 0.970370352268219, 0.9666666388511658, 0.9698148369789124, 0.9670370221138, 0.9705555438995361, 0.9696296453475952, 0.9714815020561218, 0.9700000286102295, 0.9690740704536438, 0.9733333587646484, 0.972777783870697, 0.9735184907913208, 0.9705555438995361, 0.9742592573165894, 0.977222204208374, 0.9740740656852722, 0.9766666889190674, 0.9781481623649597, 0.978518545627594, 0.9738888740539551, 0.9759259223937988, 0.9788888692855835, 0.978518545627594, 0.9775925874710083, 0.9783333539962769, 0.979629635810852, 0.9818518757820129, 0.9777777791023254, 0.978518545627594, 0.9774073958396912, 0.980555534362793, 0.9818518757820129, 0.9842592477798462, 0.9764814972877502, 0.9814814925193787, 0.9820370078086853, 0.9812963008880615, 0.9827777743339539, 0.9835185408592224, 0.9851852059364319, 0.9801852107048035, 0.9848148226737976, 0.9827777743339539, 0.982962965965271, 0.9833333492279053, 0.9844444394111633, 0.9835185408592224, 0.9824073910713196, 0.9868518710136414, 0.982962965965271, 0.9868518710136414, 0.9848148226737976, 0.9868518710136414, 0.9864814877510071, 0.985370397567749, 0.9842592477798462, 0.9872221946716309, 0.9872221946716309, 0.9870370626449585, 0.9848148226737976, 0.9890740513801575, 0.982962965965271, 0.9868518710136414, 0.987407386302948, 0.9892592430114746, 0.9870370626449585, 0.9896296262741089, 0.9877777695655823, 0.9890740513801575, 0.9872221946716309, 0.9864814877510071, 0.9879629611968994, 0.9892592430114746, 0.989814817905426, 0.9905555844306946, 0.9911110997200012, 0.9907407164573669, 0.9861111044883728, 0.9888888597488403, 0.9903703927993774, 0.9900000095367432, 0.9905555844306946, 0.989814817905426, 0.9890740513801575, 0.9905555844306946, 0.9940740466117859, 0.9859259128570557, 0.9905555844306946, 0.9894444346427917, 0.9901852011680603, 0.9907407164573669, 0.9920370578765869, 0.9907407164573669, 0.9938889145851135, 0.992222249507904, 0.9929629564285278, 0.9907407164573669, 0.9935185313224792, 0.9927777647972107, 0.992222249507904, 0.9940740466117859, 0.9911110997200012, 0.9924073815345764, 0.9925925731658936, 0.9920370578765869, 0.9933333396911621, 0.9946296215057373, 0.9914814829826355, 0.9920370578765869, 0.993148148059845, 0.9937037229537964, 0.9946296215057373, 0.9950000047683716, 0.9937037229537964, 0.9927777647972107, 0.9953703880310059, 0.9940740466117859, 0.994259238243103, 0.9924073815345764, 0.9933333396911621, 0.9950000047683716, 0.993148148059845, 0.994259238243103, 0.9935185313224792, 0.9920370578765869, 0.994259238243103, 0.9937037229537964, 0.9916666746139526, 0.9937037229537964, 0.9944444298744202, 0.9961110949516296, 0.9957407116889954, 0.9944444298744202, 0.9892592430114746, 0.9953703880310059, 0.9937037229537964, 0.992222249507904, 0.9924073815345764, 0.9962962865829468, 0.9944444298744202, 0.9927777647972107, 0.9953703880310059, 0.9957407116889954, 0.9911110997200012, 0.9948148131370544, 0.9961110949516296, 0.9957407116889954, 0.9951851963996887, 0.9962962865829468, 0.9961110949516296, 0.9957407116889954, 0.9914814829826355, 0.9961110949516296, 0.9962962865829468, 0.994259238243103, 0.9979629516601562, 0.996666669845581, 0.9959259033203125, 0.9948148131370544, 0.9961110949516296, 0.995555579662323, 0.9950000047683716, 0.9985185265541077, 0.9970370531082153, 0.997592568397522, 0.9970370531082153, 0.9964814782142639, 0.9968518614768982, 0.996666669845581, 0.9977777600288391, 0.996666669845581, 0.9968518614768982, 0.9951851963996887, 0.9959259033203125, 0.9957407116889954, 0.9953703880310059, 0.9944444298744202, 0.9959259033203125, 0.9938889145851135, 0.9970370531082153, 0.994259238243103, 0.9968518614768982, 0.9957407116889954, 0.9979629516601562, 0.9972222447395325, 0.9972222447395325, 0.9962962865829468, 0.9948148131370544, 0.9977777600288391, 0.9979629516601562, 0.9970370531082153, 0.9968518614768982, 0.9974074363708496, 0.997592568397522, 0.997592568397522, 0.996666669845581, 0.9979629516601562, 0.9979629516601562, 0.9953703880310059, 0.992222249507904, 0.9961110949516296, 0.996666669845581, 0.997592568397522, 0.9972222447395325, 0.9946296215057373, 0.996666669845581, 0.9983333349227905, 0.9951851963996887, 0.9916666746139526, 0.9983333349227905, 0.9987037181854248, 0.9972222447395325, 0.9981481432914734, 0.9979629516601562, 0.9972222447395325, 0.9977777600288391, 0.9977777600288391, 0.9979629516601562, 0.9968518614768982, 0.9972222447395325, 0.9977777600288391, 0.9964814782142639, 0.9977777600288391, 0.9987037181854248, 0.997592568397522, 0.9957407116889954, 0.9953703880310059, 0.9920370578765869, 0.9972222447395325, 0.9985185265541077, 0.9981481432914734, 0.995555579662323, 0.9970370531082153, 0.9983333349227905, 0.9985185265541077, 0.9981481432914734, 0.9987037181854248, 0.9964814782142639, 0.997592568397522, 0.9972222447395325, 0.9979629516601562, 0.996666669845581, 0.9977777600288391, 0.9981481432914734, 0.9979629516601562, 0.9977777600288391, 0.9944444298744202, 0.9981481432914734, 0.9983333349227905, 0.9981481432914734, 0.9979629516601562, 0.9953703880310059, 0.9970370531082153, 0.9985185265541077, 0.9988889098167419, 0.997592568397522, 0.9979629516601562, 0.9985185265541077, 0.9979629516601562, 0.9990741014480591, 0.9985185265541077, 0.996666669845581, 0.9974074363708496, 0.9988889098167419, 0.9953703880310059, 0.9977777600288391, 0.9987037181854248, 0.9981481432914734, 0.9961110949516296, 0.9944444298744202, 0.9983333349227905, 0.9957407116889954, 0.9959259033203125, 0.9983333349227905, 0.9983333349227905, 0.9987037181854248, 0.9988889098167419, 0.9985185265541077, 0.997592568397522, 0.9894444346427917, 0.9927777647972107, 0.9985185265541077, 0.9988889098167419, 0.9987037181854248, 0.9979629516601562, 0.9990741014480591], 'val_loss': [1.0509899854660034, 0.9390305876731873, 0.7751004099845886, 0.6994151473045349, 0.668824315071106, 0.6540974378585815, 0.6341350674629211, 0.620556116104126, 0.6116961240768433, 0.5952107906341553, 0.5847185850143433, 0.5789767503738403, 0.5694335103034973, 0.5736327171325684, 0.5564182996749878, 0.5516664385795593, 0.5415180921554565, 0.543516993522644, 0.5275362133979797, 0.5334336757659912, 0.5185500979423523, 0.5091352462768555, 0.5072264671325684, 0.5002923607826233, 0.4889432489871979, 0.4833097755908966, 0.49167096614837646, 0.4751151204109192, 0.46370065212249756, 0.45657965540885925, 0.45742103457450867, 0.44809794425964355, 0.4307721257209778, 0.42328307032585144, 0.4178963005542755, 0.41308796405792236, 0.39734989404678345, 0.4318227469921112, 0.38200703263282776, 0.38032084703445435, 0.3779243230819702, 0.3687573969364166, 0.34622177481651306, 0.33640438318252563, 0.33749908208847046, 0.3240078389644623, 0.30581215023994446, 0.3154948651790619, 0.2985773980617523, 0.28672677278518677, 0.28042179346084595, 0.27007874846458435, 0.275427907705307, 0.26850783824920654, 0.24415849149227142, 0.24220997095108032, 0.2520545721054077, 0.22973984479904175, 0.2202366441488266, 0.23670613765716553, 0.26695016026496887, 0.20842435956001282, 0.2077026516199112, 0.18923074007034302, 0.19362111389636993, 0.18373823165893555, 0.19019967317581177, 0.1803903430700302, 0.17847582697868347, 0.1717231720685959, 0.1654113382101059, 0.22028401494026184, 0.16056683659553528, 0.15790657699108124, 0.14990969002246857, 0.16773520410060883, 0.14987309277057648, 0.14827120304107666, 0.15108130872249603, 0.16360588371753693, 0.13918955624103546, 0.14544478058815002, 0.1369926929473877, 0.15174612402915955, 0.12758848071098328, 0.15707308053970337, 0.1353273242712021, 0.1217004656791687, 0.11943812668323517, 0.1246008574962616, 0.12444793432950974, 0.11386898159980774, 0.11448704451322556, 0.1149882897734642, 0.13643866777420044, 0.11329135298728943, 0.10659531503915787, 0.17735663056373596, 0.10682825744152069, 0.10306304693222046, 0.10388579219579697, 0.10731536895036697, 0.10620425641536713, 0.09842736274003983, 0.10402289777994156, 0.09918541461229324, 0.09898978471755981, 0.09658940136432648, 0.0995955541729927, 0.0979386419057846, 0.099381223320961, 0.09857244789600372, 0.08981485664844513, 0.0956583321094513, 0.0981733649969101, 0.08926867693662643, 0.10626844316720963, 0.08357817679643631, 0.08979304879903793, 0.09021718055009842, 0.0850333645939827, 0.08062009513378143, 0.09503479301929474, 0.08571059256792068, 0.08919314295053482, 0.08797383308410645, 0.08066405355930328, 0.09925676137208939, 0.07648874819278717, 0.094240203499794, 0.08247488737106323, 0.07114280015230179, 0.07739710807800293, 0.07085168361663818, 0.07782518118619919, 0.07349471002817154, 0.07380449771881104, 0.09038180112838745, 0.08608471602201462, 0.07020828872919083, 0.06662704795598984, 0.08038858324289322, 0.06669428199529648, 0.0723157748579979, 0.06375344842672348, 0.06486057490110397, 0.0629606768488884, 0.07793225347995758, 0.06481974571943283, 0.06432804465293884, 0.06795749813318253, 0.0566093772649765, 0.06265555322170258, 0.06175746023654938, 0.05992402136325836, 0.05517122894525528, 0.06171831488609314, 0.06043895334005356, 0.05623306334018707, 0.06016762554645538, 0.0748375728726387, 0.07333092391490936, 0.07394088059663773, 0.06387785077095032, 0.05517534911632538, 0.05557559058070183, 0.053652383387088776, 0.06628986448049545, 0.047986581921577454, 0.05844949558377266, 0.057363320142030716, 0.05172710865736008, 0.08219902962446213, 0.04810453578829765, 0.04771220311522484, 0.07385458052158356, 0.05074817314743996, 0.05336605757474899, 0.052002180367708206, 0.05650591477751732, 0.05523582175374031, 0.04856260120868683, 0.051856230944395065, 0.05425352230668068, 0.060396768152713776, 0.042065855115652084, 0.05000332370400429, 0.047579433768987656, 0.04696162790060043, 0.08148612082004547, 0.049067240208387375, 0.04168839007616043, 0.04086971655488014, 0.046071216464042664, 0.04512213543057442, 0.06698779761791229, 0.04816366359591484, 0.04803109169006348, 0.05253561958670616, 0.05723372474312782, 0.0528298057615757, 0.048412442207336426, 0.05280666798353195, 0.07925187796354294, 0.04672130197286606, 0.043679580092430115, 0.045137763023376465, 0.03932694345712662, 0.05846874788403511, 0.04390368238091469, 0.046298835426568985, 0.05350155755877495, 0.04934539273381233, 0.04379447177052498, 0.03743710368871689, 0.04495342820882797, 0.04101864993572235, 0.079886794090271, 0.035023435950279236, 0.04621829837560654, 0.047763291746377945, 0.037892311811447144, 0.044701676815748215, 0.04361385852098465, 0.042649947106838226, 0.04546503722667694, 0.09719126671552658, 0.049970030784606934, 0.049065858125686646, 0.041917312890291214, 0.03650789335370064, 0.04405348002910614, 0.04125448316335678, 0.034771837294101715, 0.03665256127715111, 0.04009506106376648, 0.05380015820264816, 0.041717737913131714, 0.040168263018131256, 0.04155002534389496, 0.08180132508277893, 0.04512915387749672, 0.04029132425785065, 0.037535227835178375, 0.040700070559978485, 0.041398290544748306, 0.05057361349463463, 0.054084111005067825, 0.0405801460146904, 0.04043741524219513, 0.03905612230300903, 0.0433589443564415, 0.04223036766052246, 0.03326693922281265, 0.06092064827680588, 0.0334327407181263, 0.05495304986834526, 0.029914112761616707, 0.038383401930332184, 0.04695611819624901, 0.053778789937496185, 0.03357362374663353, 0.03738285228610039, 0.044245168566703796, 0.05362597852945328, 0.04987931251525879, 0.04730718955397606, 0.058330681174993515, 0.031044961884617805, 0.04159632697701454, 0.04385058581829071, 0.043447304517030716, 0.04912244528532028, 0.043183181434869766, 0.02800094336271286, 0.027786916121840477, 0.055126652121543884, 0.03668440878391266, 0.03230424225330353, 0.033944010734558105, 0.03306993842124939, 0.04703882709145546, 0.04912228137254715, 0.05561666563153267, 0.031617578119039536, 0.0420498251914978, 0.04649766534566879, 0.041892074048519135, 0.05153004452586174, 0.04029565677046776, 0.06041262298822403, 0.03394819423556328, 0.05215786024928093, 0.034312207251787186, 0.03692164272069931, 0.02883496880531311, 0.029542211443185806, 0.04387521371245384, 0.046361226588487625, 0.05091215297579765, 0.0289132222533226, 0.040023528039455414, 0.054523009806871414, 0.04409918561577797, 0.034604694694280624, 0.03728877753019333, 0.03963431715965271, 0.039380740374326706, 0.05213727056980133, 0.07189761102199554, 0.05625734105706215, 0.02547992579638958, 0.2801717221736908, 0.029088925570249557, 0.025155317038297653, 0.035690490156412125, 0.0479104183614254, 0.029034337028861046, 0.02536364272236824, 0.04260769486427307, 0.06557175517082214, 0.06950776278972626, 0.03308426961302757, 0.035821445286273956, 0.04391901195049286, 0.05579062178730965, 0.06826944649219513, 0.02497149631381035, 0.05852599814534187, 0.035058971494436264, 0.0414215624332428, 0.037112120538949966, 0.06673502177000046, 0.04039543867111206, 0.05663257837295532, 0.02805333212018013, 0.025695011019706726, 0.025308264419436455, 0.02605961635708809, 0.023472171276807785, 0.17202086746692657, 0.03243914619088173, 0.03202282637357712, 0.04473823308944702, 0.05117755010724068, 0.039436452090740204, 0.03777798265218735, 0.030085818842053413, 0.028460776433348656, 0.04307205602526665, 0.030777903273701668, 0.026135817170143127, 0.044743072241544724, 0.03950124233961105, 0.029612047597765923, 0.08308862149715424, 0.04753343388438225, 0.031361810863018036, 0.0516250878572464, 0.029391435906291008, 0.037395354360342026, 0.053177766501903534, 0.028644807636737823, 0.07881978154182434, 0.04889276996254921, 0.046446334570646286, 0.04821796342730522, 0.038476426154375076, 0.04254952073097229, 0.04054156318306923, 0.042601656168699265, 0.05097087845206261, 0.024727409705519676, 0.03608033061027527, 0.05249236151576042, 0.03592114523053169, 0.052723340690135956, 0.046534840017557144, 0.03580467402935028, 0.029044900089502335, 0.03167454153299332, 0.025295106694102287, 0.11467932164669037, 0.09180064499378204, 0.07973591983318329, 0.030278867110610008, 0.03250132128596306, 0.03220231086015701, 0.07137805223464966, 0.039953816682100296, 0.037599217146635056, 0.033076051622629166, 0.028346620500087738, 0.04418895021080971, 0.03077538125216961, 0.057231079787015915, 0.03329450264573097, 0.05512714385986328, 0.0769110694527626, 0.03622619807720184], 'val_accuracy': [0.49166667461395264, 0.5322222113609314, 0.676111102104187, 0.70333331823349, 0.676111102104187, 0.7122222185134888, 0.7149999737739563, 0.730555534362793, 0.7038888931274414, 0.7416666746139526, 0.7605555653572083, 0.7733333110809326, 0.7711111307144165, 0.7572222352027893, 0.7838888764381409, 0.7816666960716248, 0.7955555319786072, 0.7866666913032532, 0.7977777719497681, 0.7855555415153503, 0.7988888621330261, 0.8100000023841858, 0.7977777719497681, 0.8050000071525574, 0.8144444227218628, 0.8105555772781372, 0.8005555272102356, 0.8111110925674438, 0.8172222375869751, 0.8255555629730225, 0.8244444727897644, 0.8199999928474426, 0.8294444680213928, 0.8288888931274414, 0.8383333086967468, 0.8416666388511658, 0.8461111187934875, 0.8183333277702332, 0.8611111044883728, 0.8588888645172119, 0.8561111092567444, 0.8561111092567444, 0.8722222447395325, 0.8799999952316284, 0.8655555844306946, 0.8766666650772095, 0.8844444155693054, 0.8772222399711609, 0.894444465637207, 0.9016666412353516, 0.9016666412353516, 0.9038888812065125, 0.903333306312561, 0.9083333611488342, 0.9122222065925598, 0.9183333516120911, 0.9155555367469788, 0.9233333468437195, 0.9211111068725586, 0.9188888669013977, 0.9072222113609314, 0.9305555820465088, 0.929444432258606, 0.9316666722297668, 0.9411110877990723, 0.9422222375869751, 0.9322222471237183, 0.9466666579246521, 0.9449999928474426, 0.9449999928474426, 0.9377777576446533, 0.9011111259460449, 0.9477777481079102, 0.9494444727897644, 0.9483333230018616, 0.9372222423553467, 0.945555567741394, 0.9505555629730225, 0.9483333230018616, 0.9355555772781372, 0.9555555582046509, 0.9544444680213928, 0.9549999833106995, 0.9461110830307007, 0.9555555582046509, 0.9344444274902344, 0.9522222280502319, 0.9611111283302307, 0.9611111283302307, 0.9599999785423279, 0.9599999785423279, 0.9611111283302307, 0.9594444632530212, 0.9649999737739563, 0.9477777481079102, 0.9677777886390686, 0.9661111235618591, 0.9405555725097656, 0.9616666436195374, 0.9683333039283752, 0.9661111235618591, 0.9705555438995361, 0.9666666388511658, 0.972777783870697, 0.9611111283302307, 0.971666693687439, 0.9705555438995361, 0.9694444537162781, 0.9733333587646484, 0.9694444537162781, 0.9733333587646484, 0.971666693687439, 0.9700000286102295, 0.9711111187934875, 0.9644444584846497, 0.9655555486679077, 0.9633333086967468, 0.977222204208374, 0.972777783870697, 0.971666693687439, 0.9783333539962769, 0.9783333539962769, 0.976111114025116, 0.9755555391311646, 0.9711111187934875, 0.9744444489479065, 0.9794444441795349, 0.9677777886390686, 0.9783333539962769, 0.9627777934074402, 0.9750000238418579, 0.9811111092567444, 0.9783333539962769, 0.9794444441795349, 0.9794444441795349, 0.980555534362793, 0.980555534362793, 0.9705555438995361, 0.9700000286102295, 0.9811111092567444, 0.980555534362793, 0.9783333539962769, 0.9827777743339539, 0.9811111092567444, 0.9800000190734863, 0.9800000190734863, 0.9816666841506958, 0.9794444441795349, 0.9833333492279053, 0.9822221994400024, 0.9783333539962769, 0.9855555295944214, 0.9850000143051147, 0.9844444394111633, 0.9850000143051147, 0.9833333492279053, 0.9811111092567444, 0.980555534362793, 0.9833333492279053, 0.9833333492279053, 0.9750000238418579, 0.9800000190734863, 0.9816666841506958, 0.9822221994400024, 0.9850000143051147, 0.9844444394111633, 0.9861111044883728, 0.9833333492279053, 0.9866666793823242, 0.9838888645172119, 0.9822221994400024, 0.9861111044883728, 0.9755555391311646, 0.9861111044883728, 0.9855555295944214, 0.9766666889190674, 0.9833333492279053, 0.9838888645172119, 0.9861111044883728, 0.9838888645172119, 0.9844444394111633, 0.9872221946716309, 0.9883333444595337, 0.9822221994400024, 0.9827777743339539, 0.9872221946716309, 0.9866666793823242, 0.9844444394111633, 0.9877777695655823, 0.9750000238418579, 0.9872221946716309, 0.9883333444595337, 0.9872221946716309, 0.9883333444595337, 0.9883333444595337, 0.9783333539962769, 0.9838888645172119, 0.9838888645172119, 0.9855555295944214, 0.9850000143051147, 0.9850000143051147, 0.9850000143051147, 0.9850000143051147, 0.977222204208374, 0.9883333444595337, 0.9877777695655823, 0.9888888597488403, 0.9877777695655823, 0.9838888645172119, 0.9855555295944214, 0.9900000095367432, 0.9855555295944214, 0.9877777695655823, 0.9877777695655823, 0.9900000095367432, 0.9883333444595337, 0.9877777695655823, 0.9783333539962769, 0.9894444346427917, 0.9888888597488403, 0.9861111044883728, 0.9883333444595337, 0.9911110997200012, 0.9883333444595337, 0.9905555844306946, 0.9883333444595337, 0.972777783870697, 0.9827777743339539, 0.9855555295944214, 0.9883333444595337, 0.9905555844306946, 0.9850000143051147, 0.9883333444595337, 0.9911110997200012, 0.9883333444595337, 0.9872221946716309, 0.9905555844306946, 0.9861111044883728, 0.9900000095367432, 0.9883333444595337, 0.9794444441795349, 0.9905555844306946, 0.9900000095367432, 0.9883333444595337, 0.9894444346427917, 0.9888888597488403, 0.9872221946716309, 0.9827777743339539, 0.9877777695655823, 0.9877777695655823, 0.9894444346427917, 0.9866666793823242, 0.9883333444595337, 0.9888888597488403, 0.9850000143051147, 0.9911110997200012, 0.9872221946716309, 0.9900000095367432, 0.9911110997200012, 0.9883333444595337, 0.9872221946716309, 0.9916666746139526, 0.9911110997200012, 0.9855555295944214, 0.9861111044883728, 0.9872221946716309, 0.9844444394111633, 0.9855555295944214, 0.9916666746139526, 0.9866666793823242, 0.9888888597488403, 0.9861111044883728, 0.9911110997200012, 0.9888888597488403, 0.9894444346427917, 0.9933333396911621, 0.9866666793823242, 0.9905555844306946, 0.9894444346427917, 0.9894444346427917, 0.9900000095367432, 0.9911110997200012, 0.9894444346427917, 0.9872221946716309, 0.992222249507904, 0.9872221946716309, 0.9866666793823242, 0.9894444346427917, 0.9883333444595337, 0.9877777695655823, 0.9816666841506958, 0.9877777695655823, 0.9894444346427917, 0.9905555844306946, 0.9883333444595337, 0.992222249507904, 0.9911110997200012, 0.9877777695655823, 0.9844444394111633, 0.9872221946716309, 0.992222249507904, 0.9866666793823242, 0.9894444346427917, 0.9883333444595337, 0.9911110997200012, 0.9900000095367432, 0.9900000095367432, 0.9911110997200012, 0.9877777695655823, 0.980555534362793, 0.9894444346427917, 0.9933333396911621, 0.9438889026641846, 0.9911110997200012, 0.9927777647972107, 0.9911110997200012, 0.9916666746139526, 0.9877777695655823, 0.992222249507904, 0.9911110997200012, 0.9811111092567444, 0.9822221994400024, 0.9927777647972107, 0.9883333444595337, 0.9900000095367432, 0.9850000143051147, 0.9850000143051147, 0.9927777647972107, 0.9838888645172119, 0.9894444346427917, 0.992222249507904, 0.9894444346427917, 0.9844444394111633, 0.9905555844306946, 0.9900000095367432, 0.9888888597488403, 0.9933333396911621, 0.9916666746139526, 0.9916666746139526, 0.9927777647972107, 0.9166666865348816, 0.9916666746139526, 0.9894444346427917, 0.9894444346427917, 0.9883333444595337, 0.9905555844306946, 0.9888888597488403, 0.9916666746139526, 0.992222249507904, 0.9900000095367432, 0.992222249507904, 0.9911110997200012, 0.9900000095367432, 0.9911110997200012, 0.9927777647972107, 0.9822221994400024, 0.9894444346427917, 0.992222249507904, 0.9877777695655823, 0.992222249507904, 0.992222249507904, 0.9905555844306946, 0.992222249507904, 0.9800000190734863, 0.9888888597488403, 0.9861111044883728, 0.9861111044883728, 0.9905555844306946, 0.9911110997200012, 0.9905555844306946, 0.9933333396911621, 0.9888888597488403, 0.9927777647972107, 0.9894444346427917, 0.9866666793823242, 0.9883333444595337, 0.9894444346427917, 0.9894444346427917, 0.9905555844306946, 0.9927777647972107, 0.9900000095367432, 0.9916666746139526, 0.9705555438995361, 0.9738888740539551, 0.9788888692855835, 0.9905555844306946, 0.9900000095367432, 0.9905555844306946, 0.9833333492279053, 0.9905555844306946, 0.992222249507904, 0.9900000095367432, 0.9927777647972107, 0.9844444394111633, 0.9916666746139526, 0.9877777695655823, 0.9927777647972107, 0.9911110997200012, 0.9833333492279053, 0.9905555844306946]}\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TJdiNbSYYcyl"
      },
      "source": [
        "\n",
        "prediction = model.predict_classes(X_test)\n",
        "def show_train_history(train_history, train, validation):  # 訓練集驗證準確度對epoch做圖\n",
        "    plt.plot(train_history.history[train])  # 绘制训练数据的执行结果\n",
        "    plt.plot(train_history.history[validation])  # 绘制验证数据的执行结果\n",
        "    plt.title('Train History')  # 图标题\n",
        "    plt.xlabel('epoch')  # x轴标签\n",
        "    plt.ylabel(train)  # y轴标签\n",
        "    plt.legend(['train', 'validation'], loc='upper left')  # 添加左上角图例\n",
        "    plt.show()\n",
        "\n",
        "show_train_history(train_history, 'accuracy', 'val_accuracy')\n",
        "\n",
        "show_train_history(train_history, 'loss', 'val_loss')\n",
        "\n",
        "print(pd.crosstab(y_test_label,prediction,rownames=['label'],colnames=['predict']))   # https://zhuanlan.zhihu.com/p/52368125  其他呈現交叉表方法(平均、彩色圖...)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RQ6PV4idH4Q5"
      },
      "source": [
        "from google.colab import files\n",
        "files.download(\"my_model2.h5\")"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}