{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "name": "1differentCNN_3D20C.ipynb",
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Yin-Tzu/predict_DA_reaction_product2/blob/main/3divided_straight_Cnumber/1differentCNN_3D20C.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5eeIhLEW7oPj"
      },
      "source": [
        "import zipfile\n",
        "file_dir = './2to16_Divideequally1200X200X200.zip'\n",
        "zipFile = zipfile.ZipFile(file_dir)\n",
        "for file in zipFile.namelist():\n",
        "    zipFile.extract(file, '/content')  # 解压路径\n",
        "zipFile.close()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kEo3Invc6G4h",
        "outputId": "b6155f17-0d41-436f-8f6c-2a864c70599c"
      },
      "source": [
        "!pip install psutil"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: psutil in /usr/local/lib/python3.7/dist-packages (5.4.8)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-WU18t_x6FiK"
      },
      "source": [
        "import os\n",
        "import psutil\n",
        "\n",
        "# 顯示當前 python 程式佔用的記憶體大小\n",
        "def show_memory_info(hint):\n",
        "    pid = os.getpid()\n",
        "    p = psutil.Process(pid)\n",
        "   \n",
        "    info = p.memory_full_info()\n",
        "    memory = info.uss / 1024. / 1024\n",
        "    print('{} memory used: {} MB'.format(hint, memory))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vKYbNHQHLyyi"
      },
      "source": [
        "import gc\n",
        "def generator(C, r, batch_size):\n",
        "    samples_per_epoch = C.shape[0]\n",
        "    number_of_batches = samples_per_epoch / batch_size\n",
        "    counter = 0\n",
        "    while 1:\n",
        "        X_batch = np.array(C[batch_size * counter:batch_size * (counter + 1)])\n",
        "        y_batch = np.array(r[batch_size * counter:batch_size * (counter + 1)])\n",
        "        #print(y_batch[0])\n",
        "        y_batchOneHot = tf.keras.utils.to_categorical(y_batch,3)\n",
        "        counter += 1\n",
        "        # restart counter to yeild data in the next epoch as well\n",
        "        if counter >= number_of_batches:\n",
        "            counter = 0\n",
        "        yield X_batch, y_batchOneHot\n",
        "        gc.collect()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 52
        },
        "id": "1o3bqKT6yaWM",
        "outputId": "6a8313f5-d9a8-4e10-ada9-3c3d192a9115"
      },
      "source": [
        "import zipfile\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras import layers\n",
        "import matplotlib.pyplot as plt\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import time\n",
        "from sklearn import metrics\n",
        "from sklearn.preprocessing import MinMaxScaler\n",
        "import cv2\n",
        "import copy\n",
        "from sklearn.model_selection import train_test_split\n",
        "xy=np.load('2to20_Divideequally1200X300X300.zip')\n",
        "X_train, y_train_label, X_test, y_test_label,X_valid,y_valid_label=xy['X_train'],xy['y_train_label'],xy['X_test'],xy['y_test_label'],xy['X_valid'],xy['y_valid_label']\n",
        "\"\"\"\n",
        "z = zipfile.ZipFile(\"2to6_Divideequally1200X200X200.zip\", \"r\")\n",
        "#列印zip檔案中的檔案列表\n",
        "for filename in z.namelist( ):\n",
        "  print('File:', filename)\n",
        "#讀取zip檔案中的第一個檔案\n",
        "first_file_name = z.namelist()[0]\n",
        "content = z.read(first_file_name)\n",
        "print(first_file_name)\n",
        "print(content)\n",
        "\"\"\""
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'\\nz = zipfile.ZipFile(\"2to6_Divideequally1200X200X200.zip\", \"r\")\\n#列印zip檔案中的檔案列表\\nfor filename in z.namelist( ):\\n  print(\\'File:\\', filename)\\n#讀取zip檔案中的第一個檔案\\nfirst_file_name = z.namelist()[0]\\ncontent = z.read(first_file_name)\\nprint(first_file_name)\\nprint(content)\\n'"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 2
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uJI9B8RZwCmn",
        "outputId": "7c86f657-6ec6-4ddc-a015-1de4b17b7e5c"
      },
      "source": [
        "y_TrainOneHot = tf.keras.utils.to_categorical(y_train_label)  # One-Hot编码\n",
        "y_TestOneHot = tf.keras.utils.to_categorical(y_test_label)\n",
        "y_ValidOneHot = tf.keras.utils.to_categorical(y_valid_label)  # One-Hot编码\n",
        "print(X_train.shape, y_train_label.shape, X_test.shape, y_test_label.shape,X_valid.shape,y_valid_label.shape)\n",
        "tStart = time.time()#計時開始\n",
        "\n",
        "model = tf.keras.models.Sequential()  # 调用Sequential模型\n",
        "model.add(layers.Conv2D(input_shape=(300,300, 3), filters=16, kernel_size=(3,3), kernel_initializer='TruncatedNormal', strides=1, padding='same', activation='relu', name='conv1'))  # 10*10\n",
        "model.add(layers.AveragePooling2D(pool_size=(3,3), strides=2, padding='same', name='pool1'))  # 5*5\n",
        "model.add(layers.Conv2D(filters=32, kernel_size=(3,3), kernel_initializer='TruncatedNormal', strides=1, padding='same', activation='relu', name='conv3'))  # 10*10\n",
        "model.add(layers.MaxPooling2D(pool_size=(3,3), strides=2, padding='same', name='pool3'))\n",
        "model.add(layers.Conv2D(filters=64, kernel_size=(3,3), kernel_initializer='TruncatedNormal', strides=1, padding='same', activation='relu', name='conv2'))  # 10*10\n",
        "model.add(layers.MaxPooling2D(pool_size=(3,3), strides=2, padding='same', name='pool2'))  # 5*5\n",
        "#model.add(layers.Flatten(name='flatten'))\n",
        "#model.add(layers.Dense(units=64, kernel_initializer='TruncatedNormal', activation='relu'))\n",
        "model.add(layers.GlobalAveragePooling2D())\n",
        "#model.add(layers.Dropout(0.3))  #在 0 和 1 之间浮动。需要丢弃的输入比例。\n",
        "model.add(layers.Dense(units=10,kernel_initializer='TruncatedNormal', activation='softmax'))#,input_dim=100\n",
        "\n",
        "print(model.summary())\n",
        "#batch_size = 2\n",
        "# 模型的训练 编译模型\n",
        "opt = tf.keras.optimizers.Adam(learning_rate=0.0003)# 3*3\n",
        "model.compile(loss='categorical_crossentropy', optimizer=opt, metrics=['accuracy'])  # metrics是训练和测试期间的模型评估标准。\n",
        "\n",
        "# 监控val_loss，当连续40轮变化小于0.0001时启动early stopping\n",
        "#es = tf.keras.callbacks.EarlyStopping(monitor='val_loss', mode='min', verbose=1, patience=40, min_delta=0.0001)\n",
        "\n",
        "# 训练模型\n",
        "train_history = model.fit(x=X_train, y=y_TrainOneHot, validation_data=(X_valid, y_ValidOneHot), epochs=200, batch_size=2, verbose=2)\n",
        "#train_history = model.fit_generator(generator(X_train, y_train_label, batch_size), epochs=100, steps_per_epoch=len(X_train)// batch_size,validation_data=generator(X_valid, y_valid_label, batch_size),validation_steps=len(X_valid) // batch_size, verbose=2,workers=5, use_multiprocessing=True)#validation_data=generator(X_test, y_test_label, batch_size),validation_steps=len(X_test) // batch_size\n",
        "# 查看训练过程，之前的训练步骤的值都保存在这里面。这里共有loss,accuracy,val_loss,val_accuracy四个参数\n",
        "print(train_history.history)\n",
        "\n",
        "# 將模型儲存至 HDF5 檔案中\n",
        "model.save('my_model2.h5')  # creates a HDF5 file 'my_model.h5'\n",
        "\n",
        "scores = model.evaluate(X_test,y_TestOneHot)\n",
        "#scores = model.evaluate_generator(generator(X_test, y_test_label, batch_size),steps=len(X_test)// batch_size)\n",
        "print('loss, accuracy=',scores) #显示测试准确率[1]\n",
        "\n",
        "prediction = model.predict_classes(X_test)\n",
        "# 返回预测属于某标签的概率\n",
        "y_score = model.predict_proba(X_test)\n",
        "\n",
        "t2 = time.time()#計時結束\n",
        "#列印結果\n",
        "print(\"It cost %f sec\" % (t2 - tStart))  #會自動做近位\n",
        "print(t2 - tStart)  #原型長這樣"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(7200, 300, 300, 3) (7200,) (2400, 300, 300, 3) (2400,) (2400, 300, 300, 3) (2400,)\n",
            "Model: \"sequential_1\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "conv1 (Conv2D)               (None, 300, 300, 16)      448       \n",
            "_________________________________________________________________\n",
            "batch_normalization (BatchNo (None, 300, 300, 16)      64        \n",
            "_________________________________________________________________\n",
            "pool1 (AveragePooling2D)     (None, 150, 150, 16)      0         \n",
            "_________________________________________________________________\n",
            "conv3 (Conv2D)               (None, 150, 150, 32)      4640      \n",
            "_________________________________________________________________\n",
            "batch_normalization_1 (Batch (None, 150, 150, 32)      128       \n",
            "_________________________________________________________________\n",
            "pool3 (MaxPooling2D)         (None, 75, 75, 32)        0         \n",
            "_________________________________________________________________\n",
            "conv2 (Conv2D)               (None, 75, 75, 64)        18496     \n",
            "_________________________________________________________________\n",
            "batch_normalization_2 (Batch (None, 75, 75, 64)        256       \n",
            "_________________________________________________________________\n",
            "pool2 (MaxPooling2D)         (None, 38, 38, 64)        0         \n",
            "_________________________________________________________________\n",
            "global_average_pooling2d (Gl (None, 64)                0         \n",
            "_________________________________________________________________\n",
            "dense (Dense)                (None, 10)                650       \n",
            "=================================================================\n",
            "Total params: 24,682\n",
            "Trainable params: 24,458\n",
            "Non-trainable params: 224\n",
            "_________________________________________________________________\n",
            "None\n",
            "Epoch 1/200\n",
            "3600/3600 - 21s - loss: 1.7686 - accuracy: 0.3575 - val_loss: 2.2674 - val_accuracy: 0.1371\n",
            "Epoch 2/200\n",
            "3600/3600 - 18s - loss: 1.2175 - accuracy: 0.5547 - val_loss: 1.8586 - val_accuracy: 0.2637\n",
            "Epoch 3/200\n",
            "3600/3600 - 18s - loss: 1.0170 - accuracy: 0.6172 - val_loss: 4.3169 - val_accuracy: 0.2188\n",
            "Epoch 4/200\n",
            "3600/3600 - 18s - loss: 0.9015 - accuracy: 0.6594 - val_loss: 23.7936 - val_accuracy: 0.1000\n",
            "Epoch 5/200\n",
            "3600/3600 - 18s - loss: 0.8309 - accuracy: 0.6824 - val_loss: 240.9855 - val_accuracy: 0.1000\n",
            "Epoch 6/200\n",
            "3600/3600 - 18s - loss: 0.7699 - accuracy: 0.6993 - val_loss: 6.9220 - val_accuracy: 0.1317\n",
            "Epoch 7/200\n",
            "3600/3600 - 18s - loss: 0.7230 - accuracy: 0.7154 - val_loss: 77.1781 - val_accuracy: 0.1000\n",
            "Epoch 8/200\n",
            "3600/3600 - 18s - loss: 0.6836 - accuracy: 0.7387 - val_loss: 46.4420 - val_accuracy: 0.1000\n",
            "Epoch 9/200\n",
            "3600/3600 - 18s - loss: 0.6444 - accuracy: 0.7469 - val_loss: 102.1280 - val_accuracy: 0.1000\n",
            "Epoch 10/200\n",
            "3600/3600 - 18s - loss: 0.6200 - accuracy: 0.7554 - val_loss: 61.5157 - val_accuracy: 0.1000\n",
            "Epoch 11/200\n",
            "3600/3600 - 18s - loss: 0.5934 - accuracy: 0.7661 - val_loss: 15.6571 - val_accuracy: 0.1471\n",
            "Epoch 12/200\n",
            "3600/3600 - 18s - loss: 0.5743 - accuracy: 0.7781 - val_loss: 34.8262 - val_accuracy: 0.1063\n",
            "Epoch 13/200\n",
            "3600/3600 - 18s - loss: 0.5419 - accuracy: 0.7872 - val_loss: 19.6117 - val_accuracy: 0.1104\n",
            "Epoch 14/200\n",
            "3600/3600 - 18s - loss: 0.5273 - accuracy: 0.7947 - val_loss: 23.6998 - val_accuracy: 0.2346\n",
            "Epoch 15/200\n",
            "3600/3600 - 18s - loss: 0.5111 - accuracy: 0.8008 - val_loss: 5.6734 - val_accuracy: 0.3183\n",
            "Epoch 16/200\n",
            "3600/3600 - 18s - loss: 0.4900 - accuracy: 0.8164 - val_loss: 10.1583 - val_accuracy: 0.1312\n",
            "Epoch 17/200\n",
            "3600/3600 - 18s - loss: 0.4654 - accuracy: 0.8201 - val_loss: 16.3643 - val_accuracy: 0.1254\n",
            "Epoch 18/200\n",
            "3600/3600 - 18s - loss: 0.4584 - accuracy: 0.8233 - val_loss: 11.6905 - val_accuracy: 0.1150\n",
            "Epoch 19/200\n",
            "3600/3600 - 18s - loss: 0.4405 - accuracy: 0.8328 - val_loss: 22.4934 - val_accuracy: 0.1004\n",
            "Epoch 20/200\n",
            "3600/3600 - 18s - loss: 0.4261 - accuracy: 0.8417 - val_loss: 78.7250 - val_accuracy: 0.1000\n",
            "Epoch 21/200\n",
            "3600/3600 - 18s - loss: 0.4156 - accuracy: 0.8413 - val_loss: 0.9774 - val_accuracy: 0.5896\n",
            "Epoch 22/200\n",
            "3600/3600 - 18s - loss: 0.4079 - accuracy: 0.8443 - val_loss: 3.6529 - val_accuracy: 0.4242\n",
            "Epoch 23/200\n",
            "3600/3600 - 18s - loss: 0.3974 - accuracy: 0.8544 - val_loss: 78.2928 - val_accuracy: 0.1000\n",
            "Epoch 24/200\n",
            "3600/3600 - 18s - loss: 0.3836 - accuracy: 0.8583 - val_loss: 19.3653 - val_accuracy: 0.1083\n",
            "Epoch 25/200\n",
            "3600/3600 - 18s - loss: 0.3737 - accuracy: 0.8626 - val_loss: 40.4109 - val_accuracy: 0.1004\n",
            "Epoch 26/200\n",
            "3600/3600 - 18s - loss: 0.3560 - accuracy: 0.8689 - val_loss: 3.6264 - val_accuracy: 0.3837\n",
            "Epoch 27/200\n",
            "3600/3600 - 18s - loss: 0.3482 - accuracy: 0.8756 - val_loss: 2.8161 - val_accuracy: 0.4308\n",
            "Epoch 28/200\n",
            "3600/3600 - 18s - loss: 0.3415 - accuracy: 0.8776 - val_loss: 10.7369 - val_accuracy: 0.3942\n",
            "Epoch 29/200\n",
            "3600/3600 - 18s - loss: 0.3281 - accuracy: 0.8840 - val_loss: 184.1591 - val_accuracy: 0.1000\n",
            "Epoch 30/200\n",
            "3600/3600 - 18s - loss: 0.3227 - accuracy: 0.8878 - val_loss: 2.3655 - val_accuracy: 0.4533\n",
            "Epoch 31/200\n",
            "3600/3600 - 18s - loss: 0.3103 - accuracy: 0.8918 - val_loss: 38.9413 - val_accuracy: 0.1958\n",
            "Epoch 32/200\n",
            "3600/3600 - 18s - loss: 0.2983 - accuracy: 0.8965 - val_loss: 3.5989 - val_accuracy: 0.3738\n",
            "Epoch 33/200\n",
            "3600/3600 - 18s - loss: 0.2979 - accuracy: 0.8971 - val_loss: 2.2148 - val_accuracy: 0.5246\n",
            "Epoch 34/200\n",
            "3600/3600 - 18s - loss: 0.2929 - accuracy: 0.8975 - val_loss: 4.5600 - val_accuracy: 0.3288\n",
            "Epoch 35/200\n",
            "3600/3600 - 18s - loss: 0.2827 - accuracy: 0.8997 - val_loss: 43.3046 - val_accuracy: 0.1121\n",
            "Epoch 36/200\n",
            "3600/3600 - 18s - loss: 0.2743 - accuracy: 0.9051 - val_loss: 27.3036 - val_accuracy: 0.2217\n",
            "Epoch 37/200\n",
            "3600/3600 - 18s - loss: 0.2692 - accuracy: 0.9082 - val_loss: 2.4966 - val_accuracy: 0.4871\n",
            "Epoch 38/200\n",
            "3600/3600 - 18s - loss: 0.2581 - accuracy: 0.9122 - val_loss: 13.9780 - val_accuracy: 0.2488\n",
            "Epoch 39/200\n",
            "3600/3600 - 18s - loss: 0.2523 - accuracy: 0.9158 - val_loss: 3.7088 - val_accuracy: 0.5004\n",
            "Epoch 40/200\n",
            "3600/3600 - 18s - loss: 0.2440 - accuracy: 0.9207 - val_loss: 1.9614 - val_accuracy: 0.5104\n",
            "Epoch 41/200\n",
            "3600/3600 - 18s - loss: 0.2419 - accuracy: 0.9149 - val_loss: 21.0641 - val_accuracy: 0.2396\n",
            "Epoch 42/200\n",
            "3600/3600 - 18s - loss: 0.2377 - accuracy: 0.9186 - val_loss: 36.9661 - val_accuracy: 0.1221\n",
            "Epoch 43/200\n",
            "3600/3600 - 18s - loss: 0.2276 - accuracy: 0.9233 - val_loss: 3.8008 - val_accuracy: 0.4437\n",
            "Epoch 44/200\n",
            "3600/3600 - 18s - loss: 0.2260 - accuracy: 0.9232 - val_loss: 7.2918 - val_accuracy: 0.2850\n",
            "Epoch 45/200\n",
            "3600/3600 - 18s - loss: 0.2216 - accuracy: 0.9253 - val_loss: 9.8304 - val_accuracy: 0.3713\n",
            "Epoch 46/200\n",
            "3600/3600 - 18s - loss: 0.2120 - accuracy: 0.9321 - val_loss: 11.9099 - val_accuracy: 0.2521\n",
            "Epoch 47/200\n",
            "3600/3600 - 18s - loss: 0.2044 - accuracy: 0.9340 - val_loss: 4.5405 - val_accuracy: 0.4500\n",
            "Epoch 48/200\n",
            "3600/3600 - 18s - loss: 0.1989 - accuracy: 0.9354 - val_loss: 6.2897 - val_accuracy: 0.3892\n",
            "Epoch 49/200\n",
            "3600/3600 - 18s - loss: 0.1997 - accuracy: 0.9360 - val_loss: 9.0646 - val_accuracy: 0.5092\n",
            "Epoch 50/200\n",
            "3600/3600 - 18s - loss: 0.1963 - accuracy: 0.9361 - val_loss: 13.5571 - val_accuracy: 0.3421\n",
            "Epoch 51/200\n",
            "3600/3600 - 18s - loss: 0.1934 - accuracy: 0.9367 - val_loss: 15.7207 - val_accuracy: 0.2988\n",
            "Epoch 52/200\n",
            "3600/3600 - 18s - loss: 0.1811 - accuracy: 0.9406 - val_loss: 2.2988 - val_accuracy: 0.4613\n",
            "Epoch 53/200\n",
            "3600/3600 - 18s - loss: 0.1795 - accuracy: 0.9469 - val_loss: 7.7826 - val_accuracy: 0.3817\n",
            "Epoch 54/200\n",
            "3600/3600 - 18s - loss: 0.1776 - accuracy: 0.9442 - val_loss: 2.4367 - val_accuracy: 0.5412\n",
            "Epoch 55/200\n",
            "3600/3600 - 18s - loss: 0.1677 - accuracy: 0.9485 - val_loss: 16.2741 - val_accuracy: 0.1046\n",
            "Epoch 56/200\n",
            "3600/3600 - 18s - loss: 0.1694 - accuracy: 0.9460 - val_loss: 3.7289 - val_accuracy: 0.4421\n",
            "Epoch 57/200\n",
            "3600/3600 - 18s - loss: 0.1639 - accuracy: 0.9483 - val_loss: 33.4601 - val_accuracy: 0.1304\n",
            "Epoch 58/200\n",
            "3600/3600 - 18s - loss: 0.1594 - accuracy: 0.9524 - val_loss: 130.0000 - val_accuracy: 0.1863\n",
            "Epoch 59/200\n",
            "3600/3600 - 18s - loss: 0.1472 - accuracy: 0.9557 - val_loss: 143.8320 - val_accuracy: 0.1013\n",
            "Epoch 60/200\n",
            "3600/3600 - 18s - loss: 0.1611 - accuracy: 0.9496 - val_loss: 23.2665 - val_accuracy: 0.2067\n",
            "Epoch 61/200\n",
            "3600/3600 - 18s - loss: 0.1502 - accuracy: 0.9554 - val_loss: 4.4652 - val_accuracy: 0.5229\n",
            "Epoch 62/200\n",
            "3600/3600 - 18s - loss: 0.1431 - accuracy: 0.9585 - val_loss: 5.5959 - val_accuracy: 0.3533\n",
            "Epoch 63/200\n",
            "3600/3600 - 18s - loss: 0.1403 - accuracy: 0.9583 - val_loss: 49.4642 - val_accuracy: 0.2179\n",
            "Epoch 64/200\n",
            "3600/3600 - 18s - loss: 0.1423 - accuracy: 0.9585 - val_loss: 125.4510 - val_accuracy: 0.1775\n",
            "Epoch 65/200\n",
            "3600/3600 - 18s - loss: 0.1393 - accuracy: 0.9547 - val_loss: 3.5546 - val_accuracy: 0.5208\n",
            "Epoch 66/200\n",
            "3600/3600 - 18s - loss: 0.1321 - accuracy: 0.9599 - val_loss: 8.9418 - val_accuracy: 0.3121\n",
            "Epoch 67/200\n",
            "3600/3600 - 18s - loss: 0.1324 - accuracy: 0.9611 - val_loss: 9.1713 - val_accuracy: 0.3917\n",
            "Epoch 68/200\n",
            "3600/3600 - 18s - loss: 0.1306 - accuracy: 0.9618 - val_loss: 11.2102 - val_accuracy: 0.4783\n",
            "Epoch 69/200\n",
            "3600/3600 - 18s - loss: 0.1248 - accuracy: 0.9664 - val_loss: 19.0031 - val_accuracy: 0.2050\n",
            "Epoch 70/200\n",
            "3600/3600 - 18s - loss: 0.1297 - accuracy: 0.9615 - val_loss: 4.5989 - val_accuracy: 0.4808\n",
            "Epoch 71/200\n",
            "3600/3600 - 18s - loss: 0.1108 - accuracy: 0.9699 - val_loss: 1.4893 - val_accuracy: 0.5700\n",
            "Epoch 72/200\n",
            "3600/3600 - 18s - loss: 0.1142 - accuracy: 0.9658 - val_loss: 3.5516 - val_accuracy: 0.4938\n",
            "Epoch 73/200\n",
            "3600/3600 - 18s - loss: 0.1160 - accuracy: 0.9636 - val_loss: 54.3610 - val_accuracy: 0.2125\n",
            "Epoch 74/200\n",
            "3600/3600 - 18s - loss: 0.1118 - accuracy: 0.9683 - val_loss: 8.6558 - val_accuracy: 0.2342\n",
            "Epoch 75/200\n",
            "3600/3600 - 18s - loss: 0.1168 - accuracy: 0.9617 - val_loss: 6.7661 - val_accuracy: 0.3288\n",
            "Epoch 76/200\n",
            "3600/3600 - 18s - loss: 0.1026 - accuracy: 0.9735 - val_loss: 13.9265 - val_accuracy: 0.3075\n",
            "Epoch 77/200\n",
            "3600/3600 - 18s - loss: 0.1132 - accuracy: 0.9647 - val_loss: 51.5785 - val_accuracy: 0.1358\n",
            "Epoch 78/200\n",
            "3600/3600 - 18s - loss: 0.1049 - accuracy: 0.9692 - val_loss: 2.6130 - val_accuracy: 0.5217\n",
            "Epoch 79/200\n",
            "3600/3600 - 18s - loss: 0.0977 - accuracy: 0.9729 - val_loss: 8.1387 - val_accuracy: 0.3075\n",
            "Epoch 80/200\n",
            "3600/3600 - 18s - loss: 0.1035 - accuracy: 0.9683 - val_loss: 15.2482 - val_accuracy: 0.2333\n",
            "Epoch 81/200\n",
            "3600/3600 - 18s - loss: 0.0986 - accuracy: 0.9726 - val_loss: 7.6183 - val_accuracy: 0.3133\n",
            "Epoch 82/200\n",
            "3600/3600 - 18s - loss: 0.0936 - accuracy: 0.9740 - val_loss: 10.3235 - val_accuracy: 0.3758\n",
            "Epoch 83/200\n",
            "3600/3600 - 18s - loss: 0.0951 - accuracy: 0.9729 - val_loss: 6.8581 - val_accuracy: 0.4175\n",
            "Epoch 84/200\n",
            "3600/3600 - 18s - loss: 0.0949 - accuracy: 0.9725 - val_loss: 23.9373 - val_accuracy: 0.3100\n",
            "Epoch 85/200\n",
            "3600/3600 - 18s - loss: 0.0911 - accuracy: 0.9712 - val_loss: 7.1948 - val_accuracy: 0.2808\n",
            "Epoch 86/200\n",
            "3600/3600 - 18s - loss: 0.0916 - accuracy: 0.9743 - val_loss: 2.4210 - val_accuracy: 0.5846\n",
            "Epoch 87/200\n",
            "3600/3600 - 18s - loss: 0.0844 - accuracy: 0.9768 - val_loss: 8.3502 - val_accuracy: 0.3542\n",
            "Epoch 88/200\n",
            "3600/3600 - 18s - loss: 0.0783 - accuracy: 0.9789 - val_loss: 24.4005 - val_accuracy: 0.2233\n",
            "Epoch 89/200\n",
            "3600/3600 - 18s - loss: 0.0811 - accuracy: 0.9778 - val_loss: 6.2241 - val_accuracy: 0.4629\n",
            "Epoch 90/200\n",
            "3600/3600 - 18s - loss: 0.0859 - accuracy: 0.9771 - val_loss: 6.1702 - val_accuracy: 0.4242\n",
            "Epoch 91/200\n",
            "3600/3600 - 18s - loss: 0.0807 - accuracy: 0.9778 - val_loss: 4.5622 - val_accuracy: 0.3417\n",
            "Epoch 92/200\n",
            "3600/3600 - 18s - loss: 0.0803 - accuracy: 0.9775 - val_loss: 9.1460 - val_accuracy: 0.4483\n",
            "Epoch 93/200\n",
            "3600/3600 - 18s - loss: 0.0762 - accuracy: 0.9803 - val_loss: 5.3850 - val_accuracy: 0.4858\n",
            "Epoch 94/200\n",
            "3600/3600 - 18s - loss: 0.0785 - accuracy: 0.9764 - val_loss: 2.4064 - val_accuracy: 0.5763\n",
            "Epoch 95/200\n",
            "3600/3600 - 18s - loss: 0.0775 - accuracy: 0.9774 - val_loss: 6.6145 - val_accuracy: 0.3108\n",
            "Epoch 96/200\n",
            "3600/3600 - 18s - loss: 0.0826 - accuracy: 0.9751 - val_loss: 1.4037 - val_accuracy: 0.6792\n",
            "Epoch 97/200\n",
            "3600/3600 - 18s - loss: 0.0763 - accuracy: 0.9807 - val_loss: 8.0967 - val_accuracy: 0.3654\n",
            "Epoch 98/200\n",
            "3600/3600 - 18s - loss: 0.0688 - accuracy: 0.9832 - val_loss: 15.7218 - val_accuracy: 0.2933\n",
            "Epoch 99/200\n",
            "3600/3600 - 18s - loss: 0.0701 - accuracy: 0.9826 - val_loss: 7.7188 - val_accuracy: 0.4275\n",
            "Epoch 100/200\n",
            "3600/3600 - 18s - loss: 0.0770 - accuracy: 0.9772 - val_loss: 2.4246 - val_accuracy: 0.5704\n",
            "Epoch 101/200\n",
            "3600/3600 - 18s - loss: 0.0642 - accuracy: 0.9836 - val_loss: 4.5056 - val_accuracy: 0.4954\n",
            "Epoch 102/200\n",
            "3600/3600 - 18s - loss: 0.0690 - accuracy: 0.9814 - val_loss: 19.1290 - val_accuracy: 0.2225\n",
            "Epoch 103/200\n",
            "3600/3600 - 18s - loss: 0.0645 - accuracy: 0.9840 - val_loss: 30.1880 - val_accuracy: 0.2217\n",
            "Epoch 104/200\n",
            "3600/3600 - 18s - loss: 0.0746 - accuracy: 0.9786 - val_loss: 7.2113 - val_accuracy: 0.2492\n",
            "Epoch 105/200\n",
            "3600/3600 - 18s - loss: 0.0658 - accuracy: 0.9843 - val_loss: 3.3820 - val_accuracy: 0.5658\n",
            "Epoch 106/200\n",
            "3600/3600 - 18s - loss: 0.0660 - accuracy: 0.9817 - val_loss: 4.8799 - val_accuracy: 0.4192\n",
            "Epoch 107/200\n",
            "3600/3600 - 18s - loss: 0.0665 - accuracy: 0.9837 - val_loss: 8.0228 - val_accuracy: 0.3204\n",
            "Epoch 108/200\n",
            "3600/3600 - 18s - loss: 0.0631 - accuracy: 0.9839 - val_loss: 4.8134 - val_accuracy: 0.4338\n",
            "Epoch 109/200\n",
            "3600/3600 - 18s - loss: 0.0624 - accuracy: 0.9840 - val_loss: 2.6980 - val_accuracy: 0.5504\n",
            "Epoch 110/200\n",
            "3600/3600 - 18s - loss: 0.0622 - accuracy: 0.9839 - val_loss: 1.1975 - val_accuracy: 0.7242\n",
            "Epoch 111/200\n",
            "3600/3600 - 18s - loss: 0.0662 - accuracy: 0.9828 - val_loss: 6.0753 - val_accuracy: 0.3042\n",
            "Epoch 112/200\n",
            "3600/3600 - 18s - loss: 0.0559 - accuracy: 0.9856 - val_loss: 4.4284 - val_accuracy: 0.4692\n",
            "Epoch 113/200\n",
            "3600/3600 - 18s - loss: 0.0634 - accuracy: 0.9825 - val_loss: 2.1675 - val_accuracy: 0.7083\n",
            "Epoch 114/200\n",
            "3600/3600 - 18s - loss: 0.0547 - accuracy: 0.9861 - val_loss: 1.8237 - val_accuracy: 0.5746\n",
            "Epoch 115/200\n",
            "3600/3600 - 18s - loss: 0.0616 - accuracy: 0.9835 - val_loss: 9.1156 - val_accuracy: 0.3467\n",
            "Epoch 116/200\n",
            "3600/3600 - 18s - loss: 0.0582 - accuracy: 0.9832 - val_loss: 6.2113 - val_accuracy: 0.4092\n",
            "Epoch 117/200\n",
            "3600/3600 - 18s - loss: 0.0529 - accuracy: 0.9858 - val_loss: 1.3883 - val_accuracy: 0.6158\n",
            "Epoch 118/200\n",
            "3600/3600 - 18s - loss: 0.0497 - accuracy: 0.9889 - val_loss: 13.5789 - val_accuracy: 0.3008\n",
            "Epoch 119/200\n",
            "3600/3600 - 18s - loss: 0.0547 - accuracy: 0.9851 - val_loss: 7.2770 - val_accuracy: 0.4946\n",
            "Epoch 120/200\n",
            "3600/3600 - 18s - loss: 0.0543 - accuracy: 0.9864 - val_loss: 3.5583 - val_accuracy: 0.5167\n",
            "Epoch 121/200\n",
            "3600/3600 - 18s - loss: 0.0532 - accuracy: 0.9867 - val_loss: 5.3541 - val_accuracy: 0.3317\n",
            "Epoch 122/200\n",
            "3600/3600 - 18s - loss: 0.0458 - accuracy: 0.9899 - val_loss: 5.5142 - val_accuracy: 0.4208\n",
            "Epoch 123/200\n",
            "3600/3600 - 18s - loss: 0.0547 - accuracy: 0.9865 - val_loss: 23.3400 - val_accuracy: 0.2567\n",
            "Epoch 124/200\n",
            "3600/3600 - 18s - loss: 0.0468 - accuracy: 0.9875 - val_loss: 19.9782 - val_accuracy: 0.2087\n",
            "Epoch 125/200\n",
            "3600/3600 - 18s - loss: 0.0553 - accuracy: 0.9835 - val_loss: 3.4047 - val_accuracy: 0.5263\n",
            "Epoch 126/200\n",
            "3600/3600 - 18s - loss: 0.0452 - accuracy: 0.9894 - val_loss: 17.7667 - val_accuracy: 0.2721\n",
            "Epoch 127/200\n",
            "3600/3600 - 18s - loss: 0.0469 - accuracy: 0.9887 - val_loss: 2.3065 - val_accuracy: 0.5875\n",
            "Epoch 128/200\n",
            "3600/3600 - 18s - loss: 0.0479 - accuracy: 0.9868 - val_loss: 13.1834 - val_accuracy: 0.2788\n",
            "Epoch 129/200\n",
            "3600/3600 - 18s - loss: 0.0560 - accuracy: 0.9836 - val_loss: 3.1503 - val_accuracy: 0.5883\n",
            "Epoch 130/200\n",
            "3600/3600 - 18s - loss: 0.0489 - accuracy: 0.9874 - val_loss: 5.2392 - val_accuracy: 0.3771\n",
            "Epoch 131/200\n",
            "3600/3600 - 18s - loss: 0.0495 - accuracy: 0.9865 - val_loss: 23.1198 - val_accuracy: 0.2250\n",
            "Epoch 132/200\n",
            "3600/3600 - 18s - loss: 0.0455 - accuracy: 0.9872 - val_loss: 1.4429 - val_accuracy: 0.7258\n",
            "Epoch 133/200\n",
            "3600/3600 - 18s - loss: 0.0442 - accuracy: 0.9874 - val_loss: 6.3657 - val_accuracy: 0.5000\n",
            "Epoch 134/200\n",
            "3600/3600 - 18s - loss: 0.0470 - accuracy: 0.9871 - val_loss: 3.5990 - val_accuracy: 0.5554\n",
            "Epoch 135/200\n",
            "3600/3600 - 18s - loss: 0.0467 - accuracy: 0.9878 - val_loss: 11.3401 - val_accuracy: 0.2704\n",
            "Epoch 136/200\n",
            "3600/3600 - 18s - loss: 0.0483 - accuracy: 0.9857 - val_loss: 7.2545 - val_accuracy: 0.4442\n",
            "Epoch 137/200\n",
            "3600/3600 - 18s - loss: 0.0444 - accuracy: 0.9892 - val_loss: 17.6565 - val_accuracy: 0.3408\n",
            "Epoch 138/200\n",
            "3600/3600 - 18s - loss: 0.0481 - accuracy: 0.9885 - val_loss: 1.2596 - val_accuracy: 0.7029\n",
            "Epoch 139/200\n",
            "3600/3600 - 18s - loss: 0.0420 - accuracy: 0.9892 - val_loss: 3.8020 - val_accuracy: 0.5550\n",
            "Epoch 140/200\n",
            "3600/3600 - 18s - loss: 0.0403 - accuracy: 0.9901 - val_loss: 8.7249 - val_accuracy: 0.4846\n",
            "Epoch 141/200\n",
            "3600/3600 - 18s - loss: 0.0458 - accuracy: 0.9883 - val_loss: 10.2072 - val_accuracy: 0.4396\n",
            "Epoch 142/200\n",
            "3600/3600 - 18s - loss: 0.0418 - accuracy: 0.9896 - val_loss: 14.9418 - val_accuracy: 0.3358\n",
            "Epoch 143/200\n",
            "3600/3600 - 18s - loss: 0.0418 - accuracy: 0.9899 - val_loss: 9.7345 - val_accuracy: 0.3233\n",
            "Epoch 144/200\n",
            "3600/3600 - 18s - loss: 0.0413 - accuracy: 0.9897 - val_loss: 3.2077 - val_accuracy: 0.5967\n",
            "Epoch 145/200\n",
            "3600/3600 - 18s - loss: 0.0461 - accuracy: 0.9854 - val_loss: 4.8674 - val_accuracy: 0.5183\n",
            "Epoch 146/200\n",
            "3600/3600 - 18s - loss: 0.0380 - accuracy: 0.9907 - val_loss: 2.9062 - val_accuracy: 0.6292\n",
            "Epoch 147/200\n",
            "3600/3600 - 18s - loss: 0.0379 - accuracy: 0.9922 - val_loss: 8.1724 - val_accuracy: 0.2904\n",
            "Epoch 148/200\n",
            "3600/3600 - 18s - loss: 0.0402 - accuracy: 0.9875 - val_loss: 0.7896 - val_accuracy: 0.8062\n",
            "Epoch 149/200\n",
            "3600/3600 - 18s - loss: 0.0359 - accuracy: 0.9908 - val_loss: 6.5063 - val_accuracy: 0.2829\n",
            "Epoch 150/200\n",
            "3600/3600 - 18s - loss: 0.0416 - accuracy: 0.9892 - val_loss: 4.7544 - val_accuracy: 0.4854\n",
            "Epoch 151/200\n",
            "3600/3600 - 18s - loss: 0.0442 - accuracy: 0.9879 - val_loss: 1.1419 - val_accuracy: 0.7133\n",
            "Epoch 152/200\n",
            "3600/3600 - 18s - loss: 0.0425 - accuracy: 0.9883 - val_loss: 22.1543 - val_accuracy: 0.2417\n",
            "Epoch 153/200\n",
            "3600/3600 - 18s - loss: 0.0436 - accuracy: 0.9871 - val_loss: 1.6129 - val_accuracy: 0.6687\n",
            "Epoch 154/200\n",
            "3600/3600 - 18s - loss: 0.0354 - accuracy: 0.9914 - val_loss: 5.0464 - val_accuracy: 0.4958\n",
            "Epoch 155/200\n",
            "3600/3600 - 18s - loss: 0.0373 - accuracy: 0.9903 - val_loss: 0.8629 - val_accuracy: 0.7879\n",
            "Epoch 156/200\n",
            "3600/3600 - 18s - loss: 0.0355 - accuracy: 0.9908 - val_loss: 1.4329 - val_accuracy: 0.6921\n",
            "Epoch 157/200\n",
            "3600/3600 - 18s - loss: 0.0339 - accuracy: 0.9924 - val_loss: 22.6454 - val_accuracy: 0.2887\n",
            "Epoch 158/200\n",
            "3600/3600 - 18s - loss: 0.0342 - accuracy: 0.9911 - val_loss: 1.1844 - val_accuracy: 0.7437\n",
            "Epoch 159/200\n",
            "3600/3600 - 18s - loss: 0.0338 - accuracy: 0.9914 - val_loss: 16.1475 - val_accuracy: 0.3600\n",
            "Epoch 160/200\n",
            "3600/3600 - 18s - loss: 0.0408 - accuracy: 0.9901 - val_loss: 2.0073 - val_accuracy: 0.6508\n",
            "Epoch 161/200\n",
            "3600/3600 - 18s - loss: 0.0369 - accuracy: 0.9912 - val_loss: 6.4190 - val_accuracy: 0.3121\n",
            "Epoch 162/200\n",
            "3600/3600 - 18s - loss: 0.0290 - accuracy: 0.9939 - val_loss: 2.0000 - val_accuracy: 0.6750\n",
            "Epoch 163/200\n",
            "3600/3600 - 18s - loss: 0.0338 - accuracy: 0.9918 - val_loss: 12.2812 - val_accuracy: 0.2233\n",
            "Epoch 164/200\n",
            "3600/3600 - 18s - loss: 0.0327 - accuracy: 0.9924 - val_loss: 16.7353 - val_accuracy: 0.2517\n",
            "Epoch 165/200\n",
            "3600/3600 - 18s - loss: 0.0366 - accuracy: 0.9899 - val_loss: 15.2384 - val_accuracy: 0.1704\n",
            "Epoch 166/200\n",
            "3600/3600 - 18s - loss: 0.0389 - accuracy: 0.9889 - val_loss: 11.3974 - val_accuracy: 0.4008\n",
            "Epoch 167/200\n",
            "3600/3600 - 18s - loss: 0.0370 - accuracy: 0.9894 - val_loss: 10.8965 - val_accuracy: 0.3446\n",
            "Epoch 168/200\n",
            "3600/3600 - 18s - loss: 0.0333 - accuracy: 0.9907 - val_loss: 6.7468 - val_accuracy: 0.4246\n",
            "Epoch 169/200\n",
            "3600/3600 - 18s - loss: 0.0390 - accuracy: 0.9881 - val_loss: 1.1803 - val_accuracy: 0.7283\n",
            "Epoch 170/200\n",
            "3600/3600 - 18s - loss: 0.0397 - accuracy: 0.9881 - val_loss: 9.1521 - val_accuracy: 0.2654\n",
            "Epoch 171/200\n",
            "3600/3600 - 18s - loss: 0.0304 - accuracy: 0.9926 - val_loss: 3.0995 - val_accuracy: 0.5771\n",
            "Epoch 172/200\n",
            "3600/3600 - 18s - loss: 0.0372 - accuracy: 0.9878 - val_loss: 2.5024 - val_accuracy: 0.5729\n",
            "Epoch 173/200\n",
            "3600/3600 - 18s - loss: 0.0296 - accuracy: 0.9915 - val_loss: 7.6027 - val_accuracy: 0.4554\n",
            "Epoch 174/200\n",
            "3600/3600 - 18s - loss: 0.0303 - accuracy: 0.9926 - val_loss: 4.7264 - val_accuracy: 0.4833\n",
            "Epoch 175/200\n",
            "3600/3600 - 18s - loss: 0.0395 - accuracy: 0.9875 - val_loss: 1.3539 - val_accuracy: 0.7283\n",
            "Epoch 176/200\n",
            "3600/3600 - 18s - loss: 0.0307 - accuracy: 0.9936 - val_loss: 3.0628 - val_accuracy: 0.5004\n",
            "Epoch 177/200\n",
            "3600/3600 - 18s - loss: 0.0272 - accuracy: 0.9929 - val_loss: 7.2867 - val_accuracy: 0.4396\n",
            "Epoch 178/200\n",
            "3600/3600 - 18s - loss: 0.0301 - accuracy: 0.9926 - val_loss: 11.1248 - val_accuracy: 0.3833\n",
            "Epoch 179/200\n",
            "3600/3600 - 18s - loss: 0.0300 - accuracy: 0.9935 - val_loss: 7.8145 - val_accuracy: 0.4512\n",
            "Epoch 180/200\n",
            "3600/3600 - 18s - loss: 0.0259 - accuracy: 0.9939 - val_loss: 6.1886 - val_accuracy: 0.4158\n",
            "Epoch 181/200\n",
            "3600/3600 - 18s - loss: 0.0366 - accuracy: 0.9883 - val_loss: 1.1541 - val_accuracy: 0.7567\n",
            "Epoch 182/200\n",
            "3600/3600 - 18s - loss: 0.0301 - accuracy: 0.9919 - val_loss: 37.3531 - val_accuracy: 0.2892\n",
            "Epoch 183/200\n",
            "3600/3600 - 18s - loss: 0.0276 - accuracy: 0.9932 - val_loss: 1.3614 - val_accuracy: 0.7237\n",
            "Epoch 184/200\n",
            "3600/3600 - 18s - loss: 0.0339 - accuracy: 0.9900 - val_loss: 19.7615 - val_accuracy: 0.1513\n",
            "Epoch 185/200\n",
            "3600/3600 - 18s - loss: 0.0304 - accuracy: 0.9922 - val_loss: 2.9145 - val_accuracy: 0.6029\n",
            "Epoch 186/200\n",
            "3600/3600 - 18s - loss: 0.0272 - accuracy: 0.9924 - val_loss: 3.4658 - val_accuracy: 0.4496\n",
            "Epoch 187/200\n",
            "3600/3600 - 18s - loss: 0.0339 - accuracy: 0.9903 - val_loss: 3.2999 - val_accuracy: 0.5917\n",
            "Epoch 188/200\n",
            "3600/3600 - 18s - loss: 0.0257 - accuracy: 0.9932 - val_loss: 1.2516 - val_accuracy: 0.7250\n",
            "Epoch 189/200\n",
            "3600/3600 - 18s - loss: 0.0318 - accuracy: 0.9906 - val_loss: 4.0081 - val_accuracy: 0.5412\n",
            "Epoch 190/200\n",
            "3600/3600 - 18s - loss: 0.0259 - accuracy: 0.9937 - val_loss: 3.3513 - val_accuracy: 0.5813\n",
            "Epoch 191/200\n",
            "3600/3600 - 18s - loss: 0.0280 - accuracy: 0.9929 - val_loss: 12.1103 - val_accuracy: 0.3787\n",
            "Epoch 192/200\n",
            "3600/3600 - 18s - loss: 0.0271 - accuracy: 0.9937 - val_loss: 19.8151 - val_accuracy: 0.1929\n",
            "Epoch 193/200\n",
            "3600/3600 - 18s - loss: 0.0304 - accuracy: 0.9917 - val_loss: 1.0381 - val_accuracy: 0.7867\n",
            "Epoch 194/200\n",
            "3600/3600 - 18s - loss: 0.0292 - accuracy: 0.9917 - val_loss: 2.9495 - val_accuracy: 0.5900\n",
            "Epoch 195/200\n",
            "3600/3600 - 18s - loss: 0.0232 - accuracy: 0.9947 - val_loss: 0.8640 - val_accuracy: 0.7950\n",
            "Epoch 196/200\n",
            "3600/3600 - 18s - loss: 0.0251 - accuracy: 0.9940 - val_loss: 9.8102 - val_accuracy: 0.4758\n",
            "Epoch 197/200\n",
            "3600/3600 - 18s - loss: 0.0277 - accuracy: 0.9917 - val_loss: 36.4651 - val_accuracy: 0.2313\n",
            "Epoch 198/200\n",
            "3600/3600 - 18s - loss: 0.0291 - accuracy: 0.9939 - val_loss: 1.7223 - val_accuracy: 0.6883\n",
            "Epoch 199/200\n",
            "3600/3600 - 18s - loss: 0.0291 - accuracy: 0.9926 - val_loss: 5.2358 - val_accuracy: 0.5642\n",
            "Epoch 200/200\n",
            "3600/3600 - 18s - loss: 0.0206 - accuracy: 0.9951 - val_loss: 11.3457 - val_accuracy: 0.3021\n",
            "{'loss': [1.7685760259628296, 1.2175103425979614, 1.0170118808746338, 0.9014666080474854, 0.8308802843093872, 0.769868791103363, 0.7229793071746826, 0.6835947632789612, 0.6444078087806702, 0.6200083494186401, 0.5933557748794556, 0.574306845664978, 0.541909396648407, 0.5272702574729919, 0.5110614895820618, 0.49002158641815186, 0.465424120426178, 0.45840033888816833, 0.44048163294792175, 0.4261239171028137, 0.4156040549278259, 0.40789318084716797, 0.3973654508590698, 0.3835906982421875, 0.3736893832683563, 0.35600340366363525, 0.3481896221637726, 0.3415146768093109, 0.3281366229057312, 0.32271915674209595, 0.3103308379650116, 0.29829496145248413, 0.2978614270687103, 0.29285359382629395, 0.2826874554157257, 0.27434149384498596, 0.2692050039768219, 0.25805649161338806, 0.25227949023246765, 0.2439810186624527, 0.24186746776103973, 0.23773805797100067, 0.227615088224411, 0.22600284218788147, 0.2215750515460968, 0.2120310664176941, 0.20440387725830078, 0.19893383979797363, 0.1997196227312088, 0.1962680220603943, 0.19343475997447968, 0.1810620278120041, 0.17953228950500488, 0.17755942046642303, 0.1676880568265915, 0.16935425996780396, 0.1639108657836914, 0.15936146676540375, 0.1471589207649231, 0.1610625684261322, 0.1501627266407013, 0.1430659294128418, 0.14028339087963104, 0.1423022449016571, 0.13934718072414398, 0.13205766677856445, 0.13238419592380524, 0.1305960863828659, 0.12484779953956604, 0.12974987924098969, 0.11076655238866806, 0.114173524081707, 0.11601665616035461, 0.11179646849632263, 0.11683044582605362, 0.10263676196336746, 0.11318834871053696, 0.1049470379948616, 0.0977044329047203, 0.10345827043056488, 0.09859231859445572, 0.09359565377235413, 0.09509558230638504, 0.09487628191709518, 0.09113076329231262, 0.09164242446422577, 0.08435635268688202, 0.07829277217388153, 0.08113528788089752, 0.0859026163816452, 0.08073648065328598, 0.0803154855966568, 0.07621366530656815, 0.07852965593338013, 0.07745791226625443, 0.08261195570230484, 0.07633735984563828, 0.06877552717924118, 0.07010074704885483, 0.07701887935400009, 0.06419922411441803, 0.069014772772789, 0.06447058171033859, 0.07457881420850754, 0.06581307202577591, 0.06603574752807617, 0.06647105515003204, 0.06310159713029861, 0.062425460666418076, 0.06216425821185112, 0.0661793127655983, 0.05590744689106941, 0.06335851550102234, 0.05472853034734726, 0.06156598776578903, 0.058161839842796326, 0.05291914939880371, 0.049703024327754974, 0.054688431322574615, 0.054262932389974594, 0.05324473977088928, 0.045824822038412094, 0.054731689393520355, 0.046785905957221985, 0.05534830689430237, 0.04516525939106941, 0.046851497143507004, 0.04794728755950928, 0.05599173530936241, 0.04888078197836876, 0.049511730670928955, 0.04554341733455658, 0.044167689979076385, 0.04703829437494278, 0.04674592241644859, 0.04834187030792236, 0.04438883438706398, 0.04810349643230438, 0.042044881731271744, 0.04028535261750221, 0.04584570601582527, 0.04177693650126457, 0.041786275804042816, 0.04128143563866615, 0.04613618925213814, 0.038045935332775116, 0.037874288856983185, 0.040151625871658325, 0.03587600588798523, 0.041596077382564545, 0.04421465843915939, 0.042472150176763535, 0.0435907281935215, 0.03535657003521919, 0.037326350808143616, 0.03550007566809654, 0.033860377967357635, 0.0342494361102581, 0.03383040800690651, 0.04078695923089981, 0.03694527596235275, 0.02903802879154682, 0.03376736864447594, 0.03266279399394989, 0.03664596378803253, 0.038885872811079025, 0.037035565823316574, 0.033326514065265656, 0.03901069238781929, 0.039693210273981094, 0.030384890735149384, 0.03724280372262001, 0.029568810015916824, 0.030289940536022186, 0.03954851254820824, 0.030701348558068275, 0.027151774615049362, 0.030131155624985695, 0.030010052025318146, 0.025880426168441772, 0.036644719541072845, 0.030146779492497444, 0.02762567438185215, 0.03389271721243858, 0.03042505867779255, 0.02724633179605007, 0.033864762634038925, 0.025728300213813782, 0.03184303268790245, 0.025923511013388634, 0.027979619801044464, 0.02709425613284111, 0.030425038188695908, 0.029230080544948578, 0.023198511451482773, 0.025147458538413048, 0.027664799243211746, 0.02905578352510929, 0.02912619523704052, 0.02063991315662861], 'accuracy': [0.35749998688697815, 0.554722249507904, 0.617222249507904, 0.6594444513320923, 0.6823611259460449, 0.699305534362793, 0.715416669845581, 0.7387499809265137, 0.7469444274902344, 0.7554166913032532, 0.7661111354827881, 0.7780555486679077, 0.7872222065925598, 0.7947221994400024, 0.8008333444595337, 0.8163889050483704, 0.8201388716697693, 0.8233333230018616, 0.8327777981758118, 0.8416666388511658, 0.8412500023841858, 0.8443055748939514, 0.8544444441795349, 0.8583333492279053, 0.8626388907432556, 0.8688889145851135, 0.8755555748939514, 0.8776388764381409, 0.8840277791023254, 0.8877778053283691, 0.8918055295944214, 0.8965277671813965, 0.8970833420753479, 0.8974999785423279, 0.8997222185134888, 0.9051389098167419, 0.9081944227218628, 0.9122222065925598, 0.9158333539962769, 0.9206944704055786, 0.9148610830307007, 0.9186111092567444, 0.9233333468437195, 0.9231944680213928, 0.9252777695655823, 0.9320833086967468, 0.9340277910232544, 0.9354166388511658, 0.9359722137451172, 0.9361110925674438, 0.9366666674613953, 0.9405555725097656, 0.9469444155693054, 0.9441666603088379, 0.9484722018241882, 0.945972204208374, 0.9483333230018616, 0.9523611068725586, 0.9556944370269775, 0.9495833516120911, 0.9554166793823242, 0.9584722518920898, 0.9583333134651184, 0.9584722518920898, 0.9547222256660461, 0.9598610997200012, 0.9611111283302307, 0.9618055820465088, 0.9663888812065125, 0.9615277647972107, 0.9698610901832581, 0.965833306312561, 0.9636111259460449, 0.9683333039283752, 0.9616666436195374, 0.9734722375869751, 0.964722216129303, 0.9691666960716248, 0.9729166626930237, 0.9683333039283752, 0.9726389050483704, 0.9740277528762817, 0.9729166626930237, 0.9725000262260437, 0.9712499976158142, 0.9743055701255798, 0.976805567741394, 0.9788888692855835, 0.9777777791023254, 0.9770833253860474, 0.9777777791023254, 0.9775000214576721, 0.9802777767181396, 0.9763888716697693, 0.9773610830307007, 0.9751389026641846, 0.9806944727897644, 0.9831944704055786, 0.9826388955116272, 0.977222204208374, 0.9836111068725586, 0.9813888669013977, 0.9840278029441833, 0.9786111116409302, 0.9843055605888367, 0.9816666841506958, 0.9837499856948853, 0.9838888645172119, 0.9840278029441833, 0.9838888645172119, 0.9827777743339539, 0.9855555295944214, 0.9825000166893005, 0.9861111044883728, 0.9834722280502319, 0.9831944704055786, 0.9858333468437195, 0.9888888597488403, 0.9851388931274414, 0.9863888621330261, 0.9866666793823242, 0.9898611307144165, 0.9865278005599976, 0.987500011920929, 0.9834722280502319, 0.9894444346427917, 0.9887499809265137, 0.9868055582046509, 0.9836111068725586, 0.9873611330986023, 0.9865278005599976, 0.9872221946716309, 0.9873611330986023, 0.9870833158493042, 0.9877777695655823, 0.9856944680213928, 0.9891666769981384, 0.9884722232818604, 0.9891666769981384, 0.9901388883590698, 0.9883333444595337, 0.9895833134651184, 0.9898611307144165, 0.9897222518920898, 0.9854166507720947, 0.9906944632530212, 0.992222249507904, 0.987500011920929, 0.9908333420753479, 0.9891666769981384, 0.9879166483879089, 0.9883333444595337, 0.9870833158493042, 0.9913889169692993, 0.9902777671813965, 0.9908333420753479, 0.9923611283302307, 0.9911110997200012, 0.9913889169692993, 0.9901388883590698, 0.9912499785423279, 0.9938889145851135, 0.9918055534362793, 0.9923611283302307, 0.9898611307144165, 0.9888888597488403, 0.9894444346427917, 0.9906944632530212, 0.9880555272102356, 0.9880555272102356, 0.992638885974884, 0.9877777695655823, 0.991527795791626, 0.992638885974884, 0.987500011920929, 0.9936110973358154, 0.9929166436195374, 0.992638885974884, 0.9934722185134888, 0.9938889145851135, 0.9883333444595337, 0.991944432258606, 0.9931944608688354, 0.9900000095367432, 0.992222249507904, 0.9923611283302307, 0.9902777671813965, 0.9931944608688354, 0.9905555844306946, 0.9937499761581421, 0.9929166436195374, 0.9937499761581421, 0.9916666746139526, 0.9916666746139526, 0.9947222471237183, 0.9940277934074402, 0.9916666746139526, 0.9938889145851135, 0.992638885974884, 0.9951388835906982], 'val_loss': [2.267421245574951, 1.8586305379867554, 4.316856861114502, 23.793603897094727, 240.98548889160156, 6.922040939331055, 77.17808532714844, 46.44197082519531, 102.1279525756836, 61.515663146972656, 15.657140731811523, 34.82617950439453, 19.611656188964844, 23.699758529663086, 5.673445701599121, 10.158269882202148, 16.364259719848633, 11.690488815307617, 22.493423461914062, 78.72498321533203, 0.977406919002533, 3.6528706550598145, 78.29278564453125, 19.36528205871582, 40.410945892333984, 3.626446008682251, 2.816086530685425, 10.736929893493652, 184.15907287597656, 2.3655433654785156, 38.941349029541016, 3.5989010334014893, 2.2148044109344482, 4.559961318969727, 43.304603576660156, 27.303556442260742, 2.496561288833618, 13.978015899658203, 3.708806037902832, 1.9614036083221436, 21.064109802246094, 36.96614074707031, 3.800769567489624, 7.291757106781006, 9.830377578735352, 11.909850120544434, 4.540487289428711, 6.289694786071777, 9.064607620239258, 13.557051658630371, 15.720717430114746, 2.2987704277038574, 7.782586097717285, 2.4366683959960938, 16.274076461791992, 3.7289083003997803, 33.460060119628906, 129.99996948242188, 143.83201599121094, 23.266530990600586, 4.4651665687561035, 5.595879077911377, 49.46424865722656, 125.45100402832031, 3.554572820663452, 8.941843032836914, 9.171295166015625, 11.210232734680176, 19.003135681152344, 4.5989089012146, 1.489340901374817, 3.5516178607940674, 54.361045837402344, 8.655817031860352, 6.766086101531982, 13.926527976989746, 51.57845687866211, 2.6130447387695312, 8.138669967651367, 15.248222351074219, 7.618333339691162, 10.323464393615723, 6.858113765716553, 23.9372501373291, 7.194758892059326, 2.4210140705108643, 8.350170135498047, 24.40048599243164, 6.22409200668335, 6.170244216918945, 4.562190532684326, 9.146002769470215, 5.385006427764893, 2.4063713550567627, 6.614500522613525, 1.4036649465560913, 8.09670639038086, 15.721762657165527, 7.718770503997803, 2.4246017932891846, 4.50564432144165, 19.12902069091797, 30.18804931640625, 7.211288928985596, 3.3819642066955566, 4.879913806915283, 8.022756576538086, 4.813424587249756, 2.697977304458618, 1.1974692344665527, 6.0753302574157715, 4.428410530090332, 2.167548894882202, 1.8236651420593262, 9.115643501281738, 6.2113261222839355, 1.3883211612701416, 13.578887939453125, 7.276980876922607, 3.558300733566284, 5.354069709777832, 5.5142035484313965, 23.340015411376953, 19.9781551361084, 3.4047019481658936, 17.766685485839844, 2.3064591884613037, 13.183433532714844, 3.1503260135650635, 5.239218711853027, 23.11984634399414, 1.4428573846817017, 6.36568021774292, 3.599046230316162, 11.340140342712402, 7.2544846534729, 17.656524658203125, 1.2596021890640259, 3.8020434379577637, 8.724920272827148, 10.207221984863281, 14.941781044006348, 9.734489440917969, 3.207726001739502, 4.867356777191162, 2.9061806201934814, 8.172405242919922, 0.7896323204040527, 6.506300449371338, 4.754429340362549, 1.1419265270233154, 22.154251098632812, 1.6128772497177124, 5.0463714599609375, 0.8628982305526733, 1.432943344116211, 22.6453800201416, 1.184433102607727, 16.147533416748047, 2.007255792617798, 6.4190216064453125, 1.9999544620513916, 12.281245231628418, 16.735342025756836, 15.23843765258789, 11.39736270904541, 10.896453857421875, 6.746757984161377, 1.18031644821167, 9.152053833007812, 3.0994725227355957, 2.502434492111206, 7.602712631225586, 4.726418972015381, 1.353895902633667, 3.062816619873047, 7.286675453186035, 11.12482738494873, 7.814483165740967, 6.188594341278076, 1.1541366577148438, 37.35306167602539, 1.361448884010315, 19.761524200439453, 2.91454815864563, 3.4657835960388184, 3.299931764602661, 1.2515902519226074, 4.008057117462158, 3.3513102531433105, 12.110280990600586, 19.815065383911133, 1.0380890369415283, 2.9495315551757812, 0.8639636039733887, 9.81018352508545, 36.46505355834961, 1.7222689390182495, 5.235752582550049, 11.345671653747559], 'val_accuracy': [0.13708333671092987, 0.26374998688697815, 0.21875, 0.10000000149011612, 0.10000000149011612, 0.1316666603088379, 0.10000000149011612, 0.10000000149011612, 0.10000000149011612, 0.10000000149011612, 0.1470833271741867, 0.10625000298023224, 0.11041666567325592, 0.23458333313465118, 0.31833332777023315, 0.13124999403953552, 0.12541666626930237, 0.11500000208616257, 0.10041666775941849, 0.10000000149011612, 0.5895833373069763, 0.4241666793823242, 0.10000000149011612, 0.10833333432674408, 0.10041666775941849, 0.38374999165534973, 0.4308333396911621, 0.3941666781902313, 0.10000000149011612, 0.4533333480358124, 0.19583334028720856, 0.3737500011920929, 0.5245833396911621, 0.32875001430511475, 0.1120833307504654, 0.22166666388511658, 0.4870833456516266, 0.2487500011920929, 0.5004166960716248, 0.5104166865348816, 0.2395833283662796, 0.12208333611488342, 0.4437499940395355, 0.2849999964237213, 0.3712500035762787, 0.25208333134651184, 0.44999998807907104, 0.3891666531562805, 0.5091666579246521, 0.3420833349227905, 0.29875001311302185, 0.4612500071525574, 0.3816666603088379, 0.5412499904632568, 0.10458333045244217, 0.44208332896232605, 0.1304166615009308, 0.1862500011920929, 0.10125000029802322, 0.20666666328907013, 0.5229166746139526, 0.35333332419395447, 0.21791666746139526, 0.17749999463558197, 0.5208333134651184, 0.31208333373069763, 0.3916666805744171, 0.47833332419395447, 0.20499999821186066, 0.4808333218097687, 0.5699999928474426, 0.4937500059604645, 0.21250000596046448, 0.23416666686534882, 0.32875001430511475, 0.3075000047683716, 0.13583333790302277, 0.5216666460037231, 0.3075000047683716, 0.23333333432674408, 0.31333333253860474, 0.37583333253860474, 0.41749998927116394, 0.3100000023841858, 0.28083333373069763, 0.5845833420753479, 0.3541666567325592, 0.22333332896232605, 0.46291667222976685, 0.4241666793823242, 0.34166666865348816, 0.4483333230018616, 0.4858333468437195, 0.5762500166893005, 0.3108333349227905, 0.6791666746139526, 0.36541667580604553, 0.2933333218097687, 0.42750000953674316, 0.5704166889190674, 0.49541667103767395, 0.2224999964237213, 0.22166666388511658, 0.24916666746139526, 0.565833330154419, 0.4191666543483734, 0.320416659116745, 0.4337500035762787, 0.5504166483879089, 0.7241666913032532, 0.30416667461395264, 0.46916666626930237, 0.7083333134651184, 0.5745833516120911, 0.3466666638851166, 0.4091666638851166, 0.6158333420753479, 0.3008333444595337, 0.4945833384990692, 0.5166666507720947, 0.3316666781902313, 0.4208333194255829, 0.2566666603088379, 0.20874999463558197, 0.5262500047683716, 0.2720833420753479, 0.5874999761581421, 0.2787500023841858, 0.5883333086967468, 0.37708333134651184, 0.22499999403953552, 0.7258333563804626, 0.5, 0.5554166436195374, 0.2704166769981384, 0.4441666603088379, 0.3408333361148834, 0.70291668176651, 0.5550000071525574, 0.4845833480358124, 0.43958333134651184, 0.335833340883255, 0.3233333230018616, 0.596666693687439, 0.5183333158493042, 0.6291666626930237, 0.2904166579246521, 0.8062499761581421, 0.2829166650772095, 0.4854166805744171, 0.7133333086967468, 0.24166665971279144, 0.668749988079071, 0.4958333373069763, 0.7879166603088379, 0.6920833587646484, 0.2887499928474426, 0.7437499761581421, 0.36000001430511475, 0.6508333086967468, 0.31208333373069763, 0.675000011920929, 0.22333332896232605, 0.2516666650772095, 0.1704166680574417, 0.4008333384990692, 0.34458333253860474, 0.4245833456516266, 0.7283333539962769, 0.2654166519641876, 0.5770833492279053, 0.5729166865348816, 0.4554166793823242, 0.4833333194255829, 0.7283333539962769, 0.5004166960716248, 0.43958333134651184, 0.38333332538604736, 0.45124998688697815, 0.41583332419395447, 0.7566666603088379, 0.289166659116745, 0.7237499952316284, 0.15125000476837158, 0.6029166579246521, 0.4495833218097687, 0.5916666388511658, 0.7250000238418579, 0.5412499904632568, 0.581250011920929, 0.3787499964237213, 0.1929166615009308, 0.7866666913032532, 0.5899999737739563, 0.7950000166893005, 0.47583332657814026, 0.23125000298023224, 0.6883333325386047, 0.5641666650772095, 0.3020833432674408]}\n",
            "75/75 [==============================] - 2s 19ms/step - loss: 11.3615 - accuracy: 0.3025\n",
            "loss, accuracy= [11.361477851867676, 0.30250000953674316]\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/tensorflow/python/keras/engine/sequential.py:450: UserWarning: `model.predict_classes()` is deprecated and will be removed after 2021-01-01. Please use instead:* `np.argmax(model.predict(x), axis=-1)`,   if your model does multi-class classification   (e.g. if it uses a `softmax` last-layer activation).* `(model.predict(x) > 0.5).astype(\"int32\")`,   if your model does binary classification   (e.g. if it uses a `sigmoid` last-layer activation).\n",
            "  warnings.warn('`model.predict_classes()` is deprecated and '\n",
            "/usr/local/lib/python3.7/dist-packages/tensorflow/python/keras/engine/sequential.py:425: UserWarning: `model.predict_proba()` is deprecated and will be removed after 2021-01-01. Please use `model.predict()` instead.\n",
            "  warnings.warn('`model.predict_proba()` is deprecated and '\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "It cost 3554.452813 sec\n",
            "3554.4528131484985\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ycn5UZJz-WMg"
      },
      "source": [
        "準確率loss與epoch作圖及混淆矩陣"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TJdiNbSYYcyl",
        "outputId": "602c2bbc-85eb-4bd5-9d09-e457b3e11625"
      },
      "source": [
        "\n",
        "prediction = model.predict_classes(X_test)\n",
        "def show_train_history(train_history, train, validation):  # 訓練集驗證準確度對epoch做圖\n",
        "    plt.plot(train_history.history[train])  # 绘制训练数据的执行结果\n",
        "    plt.plot(train_history.history[validation])  # 绘制验证数据的执行结果\n",
        "    plt.title('Train History')  # 图标题\n",
        "    plt.xlabel('epoch')  # x轴标签\n",
        "    plt.ylabel(train)  # y轴标签\n",
        "    plt.legend(['train', 'validation'], loc='upper left')  # 添加左上角图例\n",
        "    plt.show()\n",
        "\n",
        "show_train_history(train_history, 'accuracy', 'val_accuracy')\n",
        "\n",
        "show_train_history(train_history, 'loss', 'val_loss')\n",
        "\n",
        "print(pd.crosstab(y_test_label,prediction,rownames=['label'],colnames=['predict']))   # https://zhuanlan.zhihu.com/p/52368125  其他呈現交叉表方法(平均、彩色圖...)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/tensorflow/python/keras/engine/sequential.py:450: UserWarning: `model.predict_classes()` is deprecated and will be removed after 2021-01-01. Please use instead:* `np.argmax(model.predict(x), axis=-1)`,   if your model does multi-class classification   (e.g. if it uses a `softmax` last-layer activation).* `(model.predict(x) > 0.5).astype(\"int32\")`,   if your model does binary classification   (e.g. if it uses a `sigmoid` last-layer activation).\n",
            "  warnings.warn('`model.predict_classes()` is deprecated and '\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2ix9tmXu-T4Q",
        "outputId": "8f04c7cb-37fe-480d-c68f-b52d576f56c6"
      },
      "source": [
        "import cv2\n",
        "\n",
        "\n",
        "im = cv2.imread('20-1.png')\n",
        "im = cv2.resize(im, (300,300), interpolation=cv2.INTER_AREA)\n",
        "im=im[np.newaxis, :]\n",
        "prediction = model.predict_classes(im)\n",
        "print(prediction)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[8]\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/tensorflow/python/keras/engine/sequential.py:450: UserWarning: `model.predict_classes()` is deprecated and will be removed after 2021-01-01. Please use instead:* `np.argmax(model.predict(x), axis=-1)`,   if your model does multi-class classification   (e.g. if it uses a `softmax` last-layer activation).* `(model.predict(x) > 0.5).astype(\"int32\")`,   if your model does binary classification   (e.g. if it uses a `sigmoid` last-layer activation).\n",
            "  warnings.warn('`model.predict_classes()` is deprecated and '\n"
          ],
          "name": "stderr"
        }
      ]
    }
  ]
}